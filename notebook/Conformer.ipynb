{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conformer\n",
    "##### This notebook contains all modules for training and evaluating the Conformer model."
   ],
   "id": "eb49636374121e04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from bconformer import utils\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from Bio.PDB import PDBParser, is_aa\n",
    "from Bio.PDB.NeighborSearch import NeighborSearch\n",
    "from Bio.PDB.Selection import unfold_entities\n",
    "from functools import partial\n",
    "from torch.nn.init import trunc_normal_\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, matthews_corrcoef,\n",
    "    f1_score, precision_score, recall_score, accuracy_score,\n",
    "    brier_score_loss, log_loss\n",
    ")\n",
    "from typing import Iterable, Optional\n",
    "from timm.layers import DropPath\n",
    "from ptflops import get_model_complexity_info"
   ],
   "id": "ca17908014b7905b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "three_to_one_dict = {\n",
    "    'ALA': 'A', 'CYS': 'C', 'ASP': 'D', 'GLU': 'E',\n",
    "    'PHE': 'F', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',\n",
    "    'LYS': 'K', 'LEU': 'L', 'MET': 'M', 'ASN': 'N',\n",
    "    'PRO': 'P', 'GLN': 'Q', 'ARG': 'R', 'SER': 'S',\n",
    "    'THR': 'T', 'VAL': 'V', 'TRP': 'W', 'TYR': 'Y',\n",
    "    'SEC': 'U', 'PYL': 'O', 'ASX': 'B', 'GLX': 'Z', 'UNK': 'X'\n",
    "}"
   ],
   "id": "646e548e85ce48ab"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f25e26b6-3ab1-4cbc-96bd-a7b7618ebe4b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1080)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasta_files = \"...\" # directory containing training (or evaluating) fastas\n",
    "pdb_files = \"...\" # directory containing training (or evaluating) pdbs\n",
    "\n",
    "num_fasta = len([f for f in os.listdir(fasta_files) if f.endswith('.fasta')])\n",
    "num_pdb = len([f for f in os.listdir(pdb_files) if f.endswith('.pdb')])\n",
    "num_fasta, num_pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b041c-4b31-4ee4-b44f-38b026b47666",
   "metadata": {},
   "source": [
    "### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89eeddd5-281c-42f7-b9d5-7bd38ec1da5a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_chains_from_fasta_name(fasta_name):\n",
    "    base = fasta_name.replace('.fasta', '')\n",
    "    parts = base.split('_')\n",
    "    ag_idx = parts.index('ag')\n",
    "    ab_idx = parts.index('ab')\n",
    "    antigen_chains = parts[ag_idx+1:ab_idx]\n",
    "    antibody_chains = parts[ab_idx+1:]\n",
    "    return antigen_chains, antibody_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adda3bae-5006-44d1-a1e8-cb50a8aa5e43",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_atoms(chains):\n",
    "    return [atom for chain in chains for atom in unfold_entities(chain, 'A') if atom.element != 'H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41c820da-0234-40e4-8ac7-f14868618616",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_epitope_labels(antigen_chain_objs, antibody_chain_objs):\n",
    "    antibody_atoms = get_atoms(antibody_chain_objs)\n",
    "    ns = NeighborSearch(antibody_atoms)\n",
    "    epitope_residues = set()\n",
    "\n",
    "    for chain in antigen_chain_objs:\n",
    "        for res in chain.get_residues():\n",
    "            if not is_aa(res):\n",
    "                continue\n",
    "            for atom in res:\n",
    "                if ns.search(atom.coord, 4):\n",
    "                    epitope_residues.add((chain.id, res.id))\n",
    "                    break\n",
    "\n",
    "    labels = []\n",
    "    for chain in antigen_chain_objs:\n",
    "        for res in chain.get_residues():\n",
    "            if not is_aa(res):\n",
    "                continue\n",
    "            label_val = 1 if (chain.id, res.id) in epitope_residues else 0\n",
    "            labels.append(label_val)\n",
    "    return torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c59629a-100e-4a3e-86ac-d29c33aafa38",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def esm_embed_sequences(sequences, model, alphabet, device):\n",
    "    embeddings = []\n",
    "    for seq in sequences:\n",
    "        batch = alphabet.get_batch_converter()([(\"protein\", seq)])\n",
    "        batch_labels, batch_strs, batch_tokens = batch\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        with torch.no_grad():\n",
    "            results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "        token_embeddings = results[\"representations\"][33]\n",
    "        # Remove BOS and EOS tokens\n",
    "        seq_embedding = token_embeddings[0, 1:-1].cpu()\n",
    "        embeddings.append(seq_embedding)\n",
    "    return torch.cat(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0f3d590-c7c0-4e97-adf7-2f08d8ba6113",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class EpitopeDataset(Dataset):\n",
    "    def __init__(self, fasta_dir, pdb_dir, esm_model, esm_alphabet, device):\n",
    "        TOTAL_ANTIGEN_CHAINS = 0\n",
    "        TOTAL_ANTIBODY_CHAINS = 0\n",
    "        \n",
    "        self.fasta_dir = fasta_dir\n",
    "        self.pdb_dir = pdb_dir\n",
    "        self.esm_model = esm_model\n",
    "        self.esm_alphabet = esm_alphabet\n",
    "        self.device = device\n",
    "\n",
    "        self.fasta_files = sorted([f for f in os.listdir(fasta_dir) if f.endswith('.fasta')])\n",
    "        self.pdb_files = sorted([f for f in os.listdir(pdb_dir) if f.endswith('.pdb')])\n",
    "\n",
    "        self.antigen_len_cache = {}\n",
    "\n",
    "        total_ag = 0\n",
    "        total_ab = 0\n",
    "        for fasta_file in self.fasta_files:\n",
    "            ag_chains, ab_chains = parse_chains_from_fasta_name(fasta_file)\n",
    "            total_ag += len(ag_chains)\n",
    "            total_ab += len(ab_chains)\n",
    "\n",
    "        EpitopeDataset.TOTAL_ANTIGEN_CHAINS = total_ag\n",
    "        EpitopeDataset.TOTAL_ANTIBODY_CHAINS = total_ab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fasta_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.antigen_len_cache:\n",
    "            antigen_length = self.antigen_len_cache[idx]\n",
    "        else:\n",
    "            antigen_length = None\n",
    "\n",
    "        fasta_name = self.fasta_files[idx]\n",
    "        fasta_id = os.path.splitext(fasta_name)[0]\n",
    "\n",
    "        matched_pdb_file = None\n",
    "        for f in self.pdb_files:\n",
    "            if fasta_id in f:\n",
    "                matched_pdb_file = os.path.join(self.pdb_dir, f)\n",
    "                break\n",
    "\n",
    "        if matched_pdb_file is None:\n",
    "            raise ValueError(f\"No matching pdb file found for {fasta_name}\")\n",
    "\n",
    "        antigen_chains, antibody_chains = parse_chains_from_fasta_name(fasta_name)\n",
    "\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        structure = parser.get_structure(\"protein\", matched_pdb_file)\n",
    "        model = structure[0]\n",
    "\n",
    "        sorted_chain_ids = sorted([chain.id for chain in model])\n",
    "        assert len(sorted_chain_ids) == len(antigen_chains) + len(antibody_chains)\n",
    "\n",
    "        antigen_chain_ids = sorted_chain_ids[:len(antigen_chains)]\n",
    "        antibody_chain_ids = sorted_chain_ids[len(antigen_chains):]\n",
    "\n",
    "        antigen_chains_objs = [model[c] for c in antigen_chain_ids]\n",
    "        antibody_chains_objs = [model[c] for c in antibody_chain_ids]\n",
    "\n",
    "        # Antigen length\n",
    "        if antigen_length is None:\n",
    "            length = 0\n",
    "            for chain in antigen_chains_objs:\n",
    "                for residue in chain.get_residues():\n",
    "                    if is_aa(residue):\n",
    "                        length += 1\n",
    "            self.antigen_len_cache[idx] = length\n",
    "            antigen_length = length\n",
    "\n",
    "        # Antigen sequence\n",
    "        antigen_sequences = []\n",
    "        for chain in antigen_chains_objs:\n",
    "            seq = \"\"\n",
    "            for residue in chain.get_residues():\n",
    "                if is_aa(residue):\n",
    "                    try:\n",
    "                        resname = residue.get_resname()\n",
    "                        aa = three_to_one_dict.get(resname, 'X')\n",
    "                        seq += aa\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "            antigen_sequences.append(seq)\n",
    "\n",
    "        embedding = esm_embed_sequences(\n",
    "            antigen_sequences, self.esm_model, self.esm_alphabet, self.device\n",
    "        )\n",
    "\n",
    "        labels = get_epitope_labels(antigen_chains_objs, antibody_chains_objs)\n",
    "        mask = torch.ones(labels.shape[0], dtype=torch.bool)\n",
    "\n",
    "        return {\n",
    "            'embedding': embedding,\n",
    "            'labels': labels,\n",
    "            'mask': mask,\n",
    "            'antigen_length': antigen_length\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "797c4bde-5344-40bc-aeeb-bdab3df9e0b2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of antigen chains: 1338\n",
      "Number of antibody chains: 2160\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "esm_model, esm_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "esm_model = esm_model.to(device)\n",
    "esm_model.eval()\n",
    "\n",
    "dataset = EpitopeDataset(fasta_files, pdb_files, esm_model, esm_alphabet, device)\n",
    "\n",
    "print(\"Number of antigen chains:\", EpitopeDataset.TOTAL_ANTIGEN_CHAINS)\n",
    "print(\"Number of antibody chains:\", EpitopeDataset.TOTAL_ANTIBODY_CHAINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8b736ba-5090-44dd-850c-58f2837d701f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "max_seq_len = 1024\n",
    "\n",
    "def collate_fn_padding(batch):\n",
    "    batch_embeddings = []\n",
    "    batch_labels = []\n",
    "    batch_masks = []\n",
    "    attn_masks = []\n",
    "\n",
    "    for item in batch:\n",
    "        L = item['embedding'].shape[0]\n",
    "        pad_len = max_seq_len - L\n",
    "        if pad_len < 0:\n",
    "            continue\n",
    "\n",
    "        embedding = F.pad(item['embedding'], (0, 0, 0, pad_len), value=0)\n",
    "        labels = F.pad(item['labels'], (0, pad_len), value=-100)\n",
    "        mask = F.pad(item['mask'], (0, pad_len), value=0)\n",
    "        attn_mask = torch.cat([torch.ones(L), torch.zeros(pad_len)])\n",
    "\n",
    "        batch_embeddings.append(embedding)\n",
    "        batch_labels.append(labels)\n",
    "        batch_masks.append(mask)\n",
    "        attn_masks.append(attn_mask)\n",
    "\n",
    "    batch_embeddings = torch.stack(batch_embeddings)\n",
    "    batch_labels = torch.stack(batch_labels)\n",
    "    batch_masks = torch.stack(batch_masks)\n",
    "    attn_masks = torch.stack(attn_masks)\n",
    "\n",
    "    return {\n",
    "        \"embedding\": batch_embeddings,\n",
    "        \"labels\": batch_labels,\n",
    "        \"mask\": batch_masks,\n",
    "        \"attention_mask\": attn_masks\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1866aa22-3016-43c7-bcd5-67ac91fb92de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([8, 1024, 1280])\n",
      "Labels shape: torch.Size([8, 1024])\n",
      "Mask shape: torch.Size([8, 1024])\n",
      "Attention mask shape: torch.Size([8, 1024])\n",
      "\n",
      "=== A sequence sample ===\n",
      "Embedding shape: torch.Size([1024, 1280])\n",
      "Embedding:\n",
      "tensor([[ 0.2433, -0.2871,  0.0368,  ...,  0.1387, -0.1156, -0.1013],\n",
      "        [ 0.0637,  0.0753,  0.0817,  ...,  0.0151,  0.0629,  0.1518],\n",
      "        [-0.0257,  0.0330,  0.0512,  ...,  0.0281,  0.3724,  0.1346],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "Labels:\n",
      "tensor([   0,    0,    0,  ..., -100, -100, -100])\n",
      "Mask:\n",
      "tensor([ True,  True,  True,  ..., False, False, False])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn_padding)\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    embedding = batch[\"embedding\"]        # shape: [B, max_len, 1280]\n",
    "    labels = batch[\"labels\"]              # shape: [B, max_len]\n",
    "    mask = batch[\"mask\"]                  # shape: [B, max_len]\n",
    "    attention_mask = batch[\"attention_mask\"]  # shape: [B, max_len]\n",
    "\n",
    "    print(f\"Embedding shape: {embedding.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Mask shape: {mask.shape}\")\n",
    "    print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "\n",
    "\n",
    "    print(\"\\n=== A sequence sample ===\")\n",
    "    print(f\"Embedding shape: {embedding[0].shape}\")  # [max_len, 1280]\n",
    "    print(f\"Embedding:\\n{embedding[0]}\")\n",
    "    print(f\"Labels:\\n{labels[0]}\")\n",
    "    print(f\"Mask:\\n{mask[0]}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd62caac-48e7-45cd-8f94-4fc56a6dc9b1",
   "metadata": {},
   "source": [
    "### 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14a1f346-17ec-4fae-9483-c2d1348daab0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2da902ee-5823-4537-ade8-d661031f6a05",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ddf817e-59c9-4e9d-8e51-a3e8cc4f6c6f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=partial(nn.LayerNorm, eps=1e-6)):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bafb638f-d89f-48bb-a7ab-3f3b941c2c8d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, inplanes, outplanes, stride=1, res_conv=False, act_layer=nn.ReLU, groups=1,\n",
    "                 norm_layer=partial(nn.BatchNorm1d, eps=1e-6), drop_block=None, drop_path=None):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        expansion = 4\n",
    "        med_planes = outplanes // expansion\n",
    "\n",
    "        self.conv1 = nn.Conv1d(inplanes, med_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = norm_layer(med_planes)\n",
    "        self.act1 = act_layer(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(med_planes, med_planes, kernel_size=3, stride=stride, groups=groups, padding=1, bias=False)\n",
    "        self.bn2 = norm_layer(med_planes)\n",
    "        self.act2 = act_layer(inplace=True)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(med_planes, outplanes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = norm_layer(outplanes)\n",
    "        self.act3 = act_layer(inplace=True)\n",
    "\n",
    "        if res_conv:\n",
    "            self.residual_conv = nn.Conv1d(inplanes, outplanes, kernel_size=1, stride=stride, padding=0, bias=False)\n",
    "            self.residual_bn = norm_layer(outplanes)\n",
    "\n",
    "        self.res_conv = res_conv\n",
    "        self.drop_block = drop_block\n",
    "        self.drop_path = drop_path\n",
    "\n",
    "    def zero_init_last_bn(self):\n",
    "        nn.init.zeros_(self.bn3.weight)\n",
    "\n",
    "    def forward(self, x, x_t=None, return_x_2=True):\n",
    "        residual = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.conv2(x) if x_t is None else self.conv2(x + x_t)\n",
    "        x = self.bn2(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "        x2 = self.act2(x)\n",
    "\n",
    "        x = self.conv3(x2)\n",
    "        x = self.bn3(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "\n",
    "        if self.drop_path is not None:\n",
    "            x = self.drop_path(x)\n",
    "\n",
    "        if self.res_conv:\n",
    "            residual = self.residual_conv(residual)\n",
    "            residual = self.residual_bn(residual)\n",
    "\n",
    "        x += residual\n",
    "        x = self.act3(x)\n",
    "\n",
    "        if return_x_2:\n",
    "            return x, x2\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e46c850-f715-4b80-8b7f-884bb8ae64bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class FCUDown(nn.Module):\n",
    "    \"\"\" CNN feature maps -> Transformer patch embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inplanes, outplanes, dw_stride, act_layer=nn.GELU,\n",
    "                 norm_layer=partial(nn.LayerNorm, eps=1e-6)):\n",
    "        super(FCUDown, self).__init__()\n",
    "        self.dw_stride = dw_stride\n",
    "\n",
    "        self.conv_project = nn.Conv1d(inplanes, outplanes, kernel_size=1, stride=1, padding=0)\n",
    "        self.sample_pooling = nn.AvgPool1d(kernel_size=dw_stride, stride=dw_stride)\n",
    "\n",
    "        self.ln = norm_layer(outplanes)\n",
    "        self.act = act_layer()\n",
    "\n",
    "    def forward(self, x, x_t):\n",
    "        x = self.conv_project(x)  # [B, C, L]\n",
    "        x = self.sample_pooling(x).transpose(1, 2)  # [B, L', C_embed]\n",
    "        x = self.ln(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        x = torch.cat([x_t[:, 0][:, None, :], x], dim=1)  # [B, L'+1, C_embed]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "40ba63cf-27c1-49dc-b81a-f5854d4ab612",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class FCUUp(nn.Module):\n",
    "    \"\"\" Transformer patch embeddings -> CNN feature maps\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inplanes, outplanes, up_stride, act_layer=nn.ReLU,\n",
    "                 norm_layer=partial(nn.BatchNorm1d, eps=1e-6),):\n",
    "        super(FCUUp, self).__init__()\n",
    "\n",
    "        self.up_stride = up_stride\n",
    "        self.conv_project = nn.Conv1d(inplanes, outplanes, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn = norm_layer(outplanes)\n",
    "        self.act = act_layer()\n",
    "\n",
    "    def forward(self, x, target_length):\n",
    "        B, L, C = x.shape # x: [batch, seq_len, embed_dim]\n",
    "        # [B, L, C] -> [B, L-1, C] -> [B, C, L-1]\n",
    "        x_r = x[:, 1:].transpose(1, 2)\n",
    "        x_r = self.act(self.bn(self.conv_project(x_r)))\n",
    "\n",
    "        return F.interpolate(x_r, size=target_length, mode='linear', align_corners=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecc27ab3-bb15-4b8f-9179-435e1b3bce27",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Med_ConvBlock(nn.Module):\n",
    "    \"\"\" special case for Convblock with down sampling, adapted to 1D conv for sequences\n",
    "    \"\"\"\n",
    "    def __init__(self, inplanes, act_layer=nn.ReLU, groups=1, norm_layer=partial(nn.BatchNorm1d, eps=1e-6),\n",
    "                 drop_block=None, drop_path=None):\n",
    "\n",
    "        super(Med_ConvBlock, self).__init__()\n",
    "\n",
    "        expansion = 4\n",
    "        med_planes = inplanes // expansion\n",
    "\n",
    "        self.conv1 = nn.Conv1d(inplanes, med_planes, kernel_size=1, stride=1, padding=0, bias=False)  # 1D conv\n",
    "        self.bn1 = norm_layer(med_planes)  # 1D BN\n",
    "        self.act1 = act_layer(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(med_planes, med_planes, kernel_size=3, stride=1, groups=groups, padding=1, bias=False)  # 1D conv\n",
    "        self.bn2 = norm_layer(med_planes)  # 1D BN\n",
    "        self.act2 = act_layer(inplace=True)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(med_planes, inplanes, kernel_size=1, stride=1, padding=0, bias=False)  # 1D conv\n",
    "        self.bn3 = norm_layer(inplanes)  # 1D BN\n",
    "        self.act3 = act_layer(inplace=True)\n",
    "\n",
    "        self.drop_block = drop_block\n",
    "        self.drop_path = drop_path\n",
    "\n",
    "    def zero_init_last_bn(self):\n",
    "        nn.init.zeros_(self.bn3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "        x = self.act2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "\n",
    "        if self.drop_path is not None:\n",
    "            x = self.drop_path(x)\n",
    "\n",
    "        x += residual\n",
    "        x = self.act3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81a51858-3e3c-47b0-854d-a4bd6e556095",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ConvTransBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvTransformer basic module, adapted for 1D sequence data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inplanes, outplanes, res_conv, stride, dw_stride, embed_dim, num_heads=12, mlp_ratio=4.,\n",
    "                 qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n",
    "                 last_fusion=False, num_med_block=0, groups=1):\n",
    "\n",
    "        super(ConvTransBlock, self).__init__()\n",
    "        expansion = 4\n",
    "\n",
    "        self.cnn_block = ConvBlock(inplanes=inplanes, outplanes=outplanes, stride=stride,\n",
    "                                   res_conv=res_conv, groups=groups, \n",
    "                                   norm_layer=partial(nn.BatchNorm1d, eps=1e-6))  \n",
    "\n",
    "        if last_fusion:\n",
    "            self.fusion_block = ConvBlock(inplanes=outplanes, outplanes=outplanes, stride=1,\n",
    "                                         res_conv=True, groups=groups,\n",
    "                                         norm_layer=partial(nn.BatchNorm1d, eps=1e-6))\n",
    "        else:\n",
    "            self.fusion_block = ConvBlock(inplanes=outplanes, outplanes=outplanes, groups=groups,\n",
    "                                         norm_layer=partial(nn.BatchNorm1d, eps=1e-6))\n",
    "\n",
    "        if num_med_block > 0:\n",
    "            self.med_block = []\n",
    "            for i in range(num_med_block):\n",
    "                self.med_block.append(Med_ConvBlock(inplanes=outplanes, groups=groups,\n",
    "                                                    norm_layer=partial(nn.BatchNorm1d, eps=1e-6)))\n",
    "            self.med_block = nn.ModuleList(self.med_block)\n",
    "\n",
    "        self.squeeze_block = FCUDown(inplanes=outplanes // expansion, outplanes=embed_dim, dw_stride=dw_stride)\n",
    "\n",
    "        self.expand_block = FCUUp(inplanes=embed_dim, outplanes=outplanes // expansion, up_stride=dw_stride,\n",
    "                                 norm_layer=partial(nn.BatchNorm1d, eps=1e-6))\n",
    "\n",
    "        self.trans_block = Block(\n",
    "            dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=drop_path_rate)\n",
    "\n",
    "        self.dw_stride = dw_stride\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_med_block = num_med_block\n",
    "        self.last_fusion = last_fusion\n",
    "\n",
    "    def forward(self, x, x_t):\n",
    "        # x shape: [B, C, L]\n",
    "        x, x2 = self.cnn_block(x)  # x2 shape: [B, C_out, L_out]\n",
    "        x_st = self.squeeze_block(x2, x_t)  # x_st shape: [B, L', embed_dim]\n",
    "\n",
    "        x_t = self.trans_block(x_st + x_t)\n",
    "\n",
    "        if self.num_med_block > 0:\n",
    "            for m in self.med_block:\n",
    "                x = m(x)\n",
    "\n",
    "        x_t_r = self.expand_block(x_t, target_length=x.shape[-1])\n",
    "\n",
    "        # print(x.shape, x_t_r.shape)\n",
    "\n",
    "        x = self.fusion_block(x, x_t_r, return_x_2=False)\n",
    "\n",
    "        return x, x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96c06d03-bf3b-4f4f-ba0b-b844c35040fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Conformer(nn.Module):\n",
    "    def __init__(self, patch_size=16, in_chans=1280, num_classes=2, base_channel=320, channel_ratio=2,\n",
    "                 num_med_block=0, embed_dim=1536, depth=12, num_heads=12, mlp_ratio=2., qkv_bias=False,\n",
    "                 qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., fusion_alpha=0.5):\n",
    "        super().__init__()\n",
    "        assert depth % 3 == 0\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.fusion_alpha = fusion_alpha\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.trans_dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "\n",
    "        # classifier heads\n",
    "        self.trans_norm = nn.LayerNorm(embed_dim)\n",
    "        self.trans_token_head = nn.Linear(embed_dim, num_classes)\n",
    "        self.conv_cls_head = nn.Conv1d(2560, num_classes, kernel_size=1)\n",
    "\n",
    "        # stem\n",
    "        self.conv1 = nn.Conv1d(in_chans, 320, kernel_size=7, stride=1, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(320)\n",
    "        self.act1 = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # stage 1\n",
    "        stage_1_channel = int(base_channel * channel_ratio)\n",
    "        trans_dw_stride = 1\n",
    "        self.conv_1 = ConvBlock(inplanes=320, outplanes=stage_1_channel, res_conv=True, stride=1)\n",
    "        self.trans_patch_conv = nn.Conv1d(320, embed_dim, kernel_size=5, stride=1, padding=2)\n",
    "        self.trans_1 = Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                             qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate,\n",
    "                             attn_drop=attn_drop_rate, drop_path=self.trans_dpr[0])\n",
    "\n",
    "        # stage 2~4\n",
    "        init_stage = 2\n",
    "        fin_stage = depth // 3 + 1\n",
    "        for i in range(init_stage, fin_stage):\n",
    "            self.add_module(f'conv_trans_{i}',\n",
    "                ConvTransBlock(\n",
    "                    stage_1_channel, stage_1_channel, False, 1, dw_stride=trans_dw_stride,\n",
    "                    embed_dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias, qk_scale=qk_scale, drop_rate=drop_rate,\n",
    "                    attn_drop_rate=attn_drop_rate, drop_path_rate=self.trans_dpr[i - 1],\n",
    "                    num_med_block=num_med_block\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # stage 5~8\n",
    "        stage_2_channel = stage_1_channel * 2\n",
    "        for i in range(fin_stage, fin_stage + depth // 3):\n",
    "            in_channel = stage_1_channel if i == fin_stage else stage_2_channel\n",
    "            self.add_module(f'conv_trans_{i}',\n",
    "                ConvTransBlock(\n",
    "                    in_channel, stage_2_channel, i == fin_stage, 1, dw_stride=trans_dw_stride,\n",
    "                    embed_dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias, qk_scale=qk_scale, drop_rate=drop_rate,\n",
    "                    attn_drop_rate=attn_drop_rate, drop_path_rate=self.trans_dpr[i - 1],\n",
    "                    num_med_block=num_med_block\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # stage 9~12\n",
    "        stage_3_channel = stage_2_channel * 2\n",
    "        for i in range(fin_stage + depth // 3, fin_stage + 2 * (depth // 3)):\n",
    "            in_channel = stage_2_channel if i == fin_stage + depth // 3 else stage_3_channel\n",
    "            self.add_module(f'conv_trans_{i}',\n",
    "                ConvTransBlock(\n",
    "                    in_channel, stage_3_channel, i == fin_stage + depth // 3, 1, dw_stride=trans_dw_stride,\n",
    "                    embed_dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias, qk_scale=qk_scale, drop_rate=drop_rate,\n",
    "                    attn_drop_rate=attn_drop_rate, drop_path_rate=self.trans_dpr[i - 1],\n",
    "                    num_med_block=num_med_block,\n",
    "                    last_fusion=(i == depth)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.fin_stage = fin_stage + 2 * (depth // 3)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.)\n",
    "        elif isinstance(m, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.constant_(m.weight, 1.)\n",
    "            nn.init.constant_(m.bias, 0.)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'cls_token'}\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, L = x.shape\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "\n",
    "        x_base = self.maxpool(self.act1(self.bn1(self.conv1(x))))     # [B, 320, L]\n",
    "        x = self.conv_1(x_base, return_x_2=False)                     # [B, C, L]\n",
    "\n",
    "        x_t = self.trans_patch_conv(x_base).transpose(1, 2)          # [B, L, embed_dim]\n",
    "        x_t = torch.cat([cls_tokens, x_t], dim=1)\n",
    "        x_t = self.trans_1(x_t)\n",
    "\n",
    "        for i in range(2, self.fin_stage):\n",
    "            x, x_t = getattr(self, f'conv_trans_{i}')(x, x_t)\n",
    "\n",
    "        conv_cls = self.conv_cls_head(x)                             # [B, num_classes, L]\n",
    "        x_t = self.trans_norm(x_t)\n",
    "        tran_cls = self.trans_token_head(x_t[:, 1:, :]).transpose(1, 2)  # [B, num_classes, L]\n",
    "\n",
    "        # linear fused classifier\n",
    "        final_cls = self.fusion_alpha * conv_cls + (1 - self.fusion_alpha) * tran_cls\n",
    "        \n",
    "        return final_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a37da-b1a5-4ff4-beed-e9d1d698306f",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "808987e7-981a-479c-a746-425b3448dd1b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n",
    "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
    "                    device: torch.device, epoch: int, loss_scaler=None, max_norm: float = 0,\n",
    "                    model_ema: Optional[object] = None, mixup_fn=None,\n",
    "                    set_training_mode=True):\n",
    "    model.train(set_training_mode)\n",
    "    if hasattr(criterion, 'train'):\n",
    "        criterion.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "    print_freq = 10\n",
    "\n",
    "    for batch in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        samples = batch['embedding'].to(device)\n",
    "        targets = batch['labels'].to(device)\n",
    "        mask = batch['mask'].to(device).bool()\n",
    "        samples = samples.transpose(1, 2)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(samples)  # [B, num_classes, L]\n",
    "            loss = sequence_loss(output, targets, mask)\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        if not math.isfinite(loss_value):\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "\n",
    "        metric_logger.update(loss=loss_value)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8a80b46-045b-4cd7-8576-3267cd0dc8eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(data_loader, model, device, threshold=0.3):\n",
    "    model.eval()\n",
    "    true_positives = 0\n",
    "    union_positives = 0\n",
    "\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    sample_ious = []\n",
    "\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = \"Eval:\"\n",
    "    print_freq = 10\n",
    "    metric_logger.add_meter(\"mean_iou\", utils.SmoothedValue(window_size=1, fmt='{value:.4f}'))\n",
    "\n",
    "    for batch in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        samples = batch['embedding'].to(device)\n",
    "        targets = batch['labels'].to(device)\n",
    "        mask = batch['mask'].to(device).bool()\n",
    "        samples = samples.transpose(1, 2)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(samples)\n",
    "            probs = torch.softmax(output, dim=1)[:, 1, :]\n",
    "            preds = (probs > threshold).long()\n",
    "            preds = preds.masked_fill(~mask, 0)\n",
    "\n",
    "            for i in range(samples.shape[0]):\n",
    "                pred_i = preds[i]\n",
    "                target_i = targets[i]\n",
    "                mask_i = mask[i]\n",
    "\n",
    "                tp_i = ((pred_i == 1) & (target_i == 1) & mask_i).sum().item()\n",
    "                union_i = (((pred_i == 1) | (target_i == 1)) & mask_i).sum().item()\n",
    "                iou_i = tp_i / union_i if union_i > 0 else 0.0\n",
    "                sample_ious.append(iou_i)\n",
    "\n",
    "            tp = ((preds == 1) & (targets == 1) & mask).sum().item()\n",
    "            union = (((preds == 1) | (targets == 1)) & mask).sum().item()\n",
    "            true_positives += tp\n",
    "            union_positives += union\n",
    "\n",
    "            all_probs.append(probs[mask].cpu())\n",
    "            all_preds.append(preds[mask].cpu())\n",
    "            all_targets.append(targets[mask].cpu())\n",
    "\n",
    "            mean_iou_so_far = sum(sample_ious) / len(sample_ious)\n",
    "            metric_logger.update(mean_iou=mean_iou_so_far)\n",
    "\n",
    "    metric_logger.synchronize_between_processes()\n",
    "\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "    agiou = true_positives / union_positives if union_positives > 0 else 0.0\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(all_targets, all_probs)\n",
    "    except:\n",
    "        auc = float('nan')\n",
    "\n",
    "    try:\n",
    "        pr_auc = average_precision_score(all_targets, all_probs)\n",
    "    except:\n",
    "        pr_auc = float('nan')\n",
    "\n",
    "    try:\n",
    "        pcc = np.corrcoef(all_probs, all_targets)[0, 1]\n",
    "    except:\n",
    "        pcc = float('nan')\n",
    "\n",
    "    try:\n",
    "        brier = brier_score_loss(all_targets, all_probs)\n",
    "    except:\n",
    "        brier = float('nan')\n",
    "\n",
    "    try:\n",
    "        bce = log_loss(all_targets, all_probs, labels=[0, 1])\n",
    "    except:\n",
    "        bce = float('nan')\n",
    "\n",
    "    results = {\n",
    "        \"AgIoU\": round(agiou, 4),\n",
    "        \"Precision\": round(precision_score(all_targets, all_preds, zero_division=0), 4),\n",
    "        \"Recall\": round(recall_score(all_targets, all_preds, zero_division=0), 4),\n",
    "        \"F1\": round(f1_score(all_targets, all_preds, zero_division=0), 4),\n",
    "        \"MCC\": round(matthews_corrcoef(all_targets, all_preds), 4),\n",
    "        \"Accuracy\": round(accuracy_score(all_targets, all_preds), 4),\n",
    "        \"AUC\": round(auc, 4),\n",
    "        \"PR-AUC\": round(pr_auc, 4),\n",
    "        \"PCC\": round(pcc, 4),\n",
    "        \"Brier\": round(brier, 4),\n",
    "        \"BCE\": round(bce, 4)\n",
    "    }\n",
    "\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    for k, v in results.items():\n",
    "        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "    return results, sample_ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "946ebf00-0089-47c8-9c42-7d6f44c6ae87",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sequence_loss(pred, target, mask):\n",
    "    \"\"\"\n",
    "    pred: [B, C, L]\n",
    "    target: [B, L]\n",
    "    mask: [B, L] (bool)\n",
    "    \"\"\"\n",
    "    B, C, L = pred.shape\n",
    "    pred = pred.transpose(1, 2).reshape(-1, C)      # [B*L, C]\n",
    "    target = target.reshape(-1)                     # [B*L]\n",
    "    mask = mask.reshape(-1)                         # [B*L], bool\n",
    "\n",
    "    loss = F.cross_entropy(pred, target, reduction='none')  # [B*L]\n",
    "    loss = loss[mask].mean()  # only valid positions\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6dac9df4-4067-4789-8e70-f34e8db66feb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def criterion(output, target, mask):\n",
    "    return sequence_loss(output, target, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a497559-11a6-432e-ae70-695f353c81fa",
   "metadata": {},
   "source": [
    "### 3.1 Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d8530a-8122-441e-b01f-fa2e2d7b44c1",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a11960e-1dc3-4069-8807-9329926d95c7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conformer(\n",
      "  302.24 M, 99.999% Params, 309.79 GMac, 99.956% MACs, \n",
      "  (trans_norm): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-05, elementwise_affine=True)\n",
      "  (trans_token_head): Linear(3.07 k, 0.001% Params, 3.15 MMac, 0.001% MACs, in_features=1536, out_features=2, bias=True)\n",
      "  (conv_cls_head): Conv1d(5.12 k, 0.002% Params, 5.24 MMac, 0.002% MACs, 2560, 2, kernel_size=(1,), stride=(1,))\n",
      "  (conv1): Conv1d(2.87 M, 0.949% Params, 2.94 GMac, 0.947% MACs, 1280, 320, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "  (bn1): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act1): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, inplace=True)\n",
      "  (maxpool): MaxPool1d(0, 0.000% Params, 327.68 KMac, 0.000% MACs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "  (conv_1): ConvBlock(\n",
      "    438.4 k, 0.145% Params, 449.9 MMac, 0.145% MACs, \n",
      "    (conv1): Conv1d(51.2 k, 0.017% Params, 52.43 MMac, 0.017% MACs, 320, 160, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (bn1): BatchNorm1d(320, 0.000% Params, 327.68 KMac, 0.000% MACs, 160, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act1): ReLU(0, 0.000% Params, 163.84 KMac, 0.000% MACs, inplace=True)\n",
      "    (conv2): Conv1d(76.8 k, 0.025% Params, 78.64 MMac, 0.025% MACs, 160, 160, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "    (bn2): BatchNorm1d(320, 0.000% Params, 327.68 KMac, 0.000% MACs, 160, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act2): ReLU(0, 0.000% Params, 163.84 KMac, 0.000% MACs, inplace=True)\n",
      "    (conv3): Conv1d(102.4 k, 0.034% Params, 104.86 MMac, 0.034% MACs, 160, 640, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (bn3): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act3): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "    (residual_conv): Conv1d(204.8 k, 0.068% Params, 209.72 MMac, 0.068% MACs, 320, 640, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (residual_bn): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (trans_patch_conv): Conv1d(2.46 M, 0.814% Params, 2.52 GMac, 0.812% MACs, 320, 1536, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (trans_1): Block(\n",
      "    18.89 M, 6.249% Params, 19.36 GMac, 6.246% MACs, \n",
      "    (norm1): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      9.44 M, 3.123% Params, 9.67 GMac, 3.122% MACs, \n",
      "      (qkv): Linear(7.08 M, 2.342% Params, 7.25 GMac, 2.341% MACs, in_features=1536, out_features=4608, bias=False)\n",
      "      (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      (proj): Linear(2.36 M, 0.781% Params, 2.42 GMac, 0.781% MACs, in_features=1536, out_features=1536, bias=True)\n",
      "      (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "    (norm2): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      9.44 M, 3.124% Params, 9.68 GMac, 3.124% MACs, \n",
      "      (fc1): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.562% MACs, in_features=1536, out_features=3072, bias=True)\n",
      "      (act): GELU(0, 0.000% Params, 3.15 MMac, 0.001% MACs, approximate='none')\n",
      "      (fc2): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.561% MACs, in_features=3072, out_features=1536, bias=True)\n",
      "      (drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (conv_trans_2): ConvTransBlock(\n",
      "    19.95 M, 6.601% Params, 20.45 GMac, 6.599% MACs, \n",
      "    (cnn_block): ConvBlock(\n",
      "      283.52 k, 0.094% Params, 291.31 MMac, 0.094% MACs, \n",
      "      (conv1): Conv1d(102.4 k, 0.034% Params, 104.86 MMac, 0.034% MACs, 640, 160, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(320, 0.000% Params, 327.68 KMac, 0.000% MACs, 160, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 163.84 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(76.8 k, 0.025% Params, 78.64 MMac, 0.025% MACs, 160, 160, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(320, 0.000% Params, 327.68 KMac, 0.000% MACs, 160, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 163.84 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(102.4 k, 0.034% Params, 104.86 MMac, 0.034% MACs, 160, 640, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "    )\n",
      "    (fusion_block): ConvBlock(\n",
      "      283.52 k, 0.094% Params, 291.31 MMac, 0.094% MACs, \n",
      "      (conv1): Conv1d(102.4 k, 0.034% Params, 104.86 MMac, 0.034% MACs, 640, 160, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(320, 0.000% Params, 327.68 KMac, 0.000% MACs, 160, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 163.84 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(76.8 k, 0.025% Params, 78.64 MMac, 0.025% MACs, 160, 160, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(320, 0.000% Params, 327.68 KMac, 0.000% MACs, 160, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 163.84 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(102.4 k, 0.034% Params, 104.86 MMac, 0.034% MACs, 160, 640, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "    )\n",
      "    (squeeze_block): FCUDown(\n",
      "      250.37 k, 0.083% Params, 257.95 MMac, 0.083% MACs, \n",
      "      (conv_project): Conv1d(247.3 k, 0.082% Params, 253.23 MMac, 0.082% MACs, 160, 1536, kernel_size=(1,), stride=(1,))\n",
      "      (sample_pooling): AvgPool1d(0, 0.000% Params, 1.57 MMac, 0.001% MACs, kernel_size=(1,), stride=(1,), padding=(0,))\n",
      "      (ln): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (act): GELU(0, 0.000% Params, 1.57 MMac, 0.001% MACs, approximate='none')\n",
      "    )\n",
      "    (expand_block): FCUUp(\n",
      "      246.24 k, 0.081% Params, 252.31 MMac, 0.081% MACs, \n",
      "      (conv_project): Conv1d(245.92 k, 0.081% Params, 251.82 MMac, 0.081% MACs, 1536, 160, kernel_size=(1,), stride=(1,))\n",
      "      (bn): BatchNorm1d(320, 0.000% Params, 327.68 KMac, 0.000% MACs, 160, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(0, 0.000% Params, 163.84 KMac, 0.000% MACs, )\n",
      "    )\n",
      "    (trans_block): Block(\n",
      "      18.89 M, 6.249% Params, 19.36 GMac, 6.246% MACs, \n",
      "      (norm1): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        9.44 M, 3.123% Params, 9.67 GMac, 3.122% MACs, \n",
      "        (qkv): Linear(7.08 M, 2.342% Params, 7.25 GMac, 2.341% MACs, in_features=1536, out_features=4608, bias=False)\n",
      "        (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "        (proj): Linear(2.36 M, 0.781% Params, 2.42 GMac, 0.781% MACs, in_features=1536, out_features=1536, bias=True)\n",
      "        (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "      (norm2): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        9.44 M, 3.124% Params, 9.68 GMac, 3.124% MACs, \n",
      "        (fc1): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.562% MACs, in_features=1536, out_features=3072, bias=True)\n",
      "        (act): GELU(0, 0.000% Params, 3.15 MMac, 0.001% MACs, approximate='none')\n",
      "        (fc2): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.561% MACs, in_features=3072, out_features=1536, bias=True)\n",
      "        (drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_trans_3): ConvTransBlock(\n",
      "    19.95 M, 6.601% Params, 20.45 GMac, 6.599% MACs, \n",
      "    (cnn_block): ConvBlock(\n",
      "      283.52 k, 0.094% Params, 291.31 MMac, 0.094% MACs, \n",
      "      (conv1): Conv1d(102.4 k, 0.034% Params, 104.86 MMac, 0.034% MACs, 640, 160, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(320, 0.000% Params, 327.68 KMac, 0.000% MACs, 160, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 163.84 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(76.8 k, 0.025% Params, 78.64 MMac, 0.025% MACs, 160, 160, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(320, 0.000% Params, 327.68 KMac, 0.000% MACs, 160, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 163.84 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(102.4 k, 0.034% Params, 104.86 MMac, 0.034% MACs, 160, 640, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "    )\n",
      "    (fusion_block): ConvBlock(\n",
      "      283.52 k, 0.094% Params, 291.31 MMac, 0.094% MACs, \n",
      "      (conv1): Conv1d(102.4 k, 0.034% Params, 104.86 MMac, 0.034% MACs, 640, 160, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(320, 0.000% Params, 327.68 KMac, 0.000% MACs, 160, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 163.84 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(76.8 k, 0.025% Params, 78.64 MMac, 0.025% MACs, 160, 160, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(320, 0.000% Params, 327.68 KMac, 0.000% MACs, 160, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 163.84 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(102.4 k, 0.034% Params, 104.86 MMac, 0.034% MACs, 160, 640, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "    )\n",
      "    (squeeze_block): FCUDown(\n",
      "      250.37 k, 0.083% Params, 257.95 MMac, 0.083% MACs, \n",
      "      (conv_project): Conv1d(247.3 k, 0.082% Params, 253.23 MMac, 0.082% MACs, 160, 1536, kernel_size=(1,), stride=(1,))\n",
      "      (sample_pooling): AvgPool1d(0, 0.000% Params, 1.57 MMac, 0.001% MACs, kernel_size=(1,), stride=(1,), padding=(0,))\n",
      "      (ln): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (act): GELU(0, 0.000% Params, 1.57 MMac, 0.001% MACs, approximate='none')\n",
      "    )\n",
      "    (expand_block): FCUUp(\n",
      "      246.24 k, 0.081% Params, 252.31 MMac, 0.081% MACs, \n",
      "      (conv_project): Conv1d(245.92 k, 0.081% Params, 251.82 MMac, 0.081% MACs, 1536, 160, kernel_size=(1,), stride=(1,))\n",
      "      (bn): BatchNorm1d(320, 0.000% Params, 327.68 KMac, 0.000% MACs, 160, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(0, 0.000% Params, 163.84 KMac, 0.000% MACs, )\n",
      "    )\n",
      "    (trans_block): Block(\n",
      "      18.89 M, 6.249% Params, 19.36 GMac, 6.246% MACs, \n",
      "      (norm1): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        9.44 M, 3.123% Params, 9.67 GMac, 3.122% MACs, \n",
      "        (qkv): Linear(7.08 M, 2.342% Params, 7.25 GMac, 2.341% MACs, in_features=1536, out_features=4608, bias=False)\n",
      "        (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "        (proj): Linear(2.36 M, 0.781% Params, 2.42 GMac, 0.781% MACs, in_features=1536, out_features=1536, bias=True)\n",
      "        (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "      (norm2): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        9.44 M, 3.124% Params, 9.68 GMac, 3.124% MACs, \n",
      "        (fc1): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.562% MACs, in_features=1536, out_features=3072, bias=True)\n",
      "        (act): GELU(0, 0.000% Params, 3.15 MMac, 0.001% MACs, approximate='none')\n",
      "        (fc2): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.561% MACs, in_features=3072, out_features=1536, bias=True)\n",
      "        (drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_trans_4): ConvTransBlock(\n",
      "    19.95 M, 6.601% Params, 20.45 GMac, 6.599% MACs, \n",
      "    (cnn_block): ConvBlock(\n",
      "      283.52 k, 0.094% Params, 291.31 MMac, 0.094% MACs, \n",
      "      (conv1): Conv1d(102.4 k, 0.034% Params, 104.86 MMac, 0.034% MACs, 640, 160, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(320, 0.000% Params, 327.68 KMac, 0.000% MACs, 160, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 163.84 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(76.8 k, 0.025% Params, 78.64 MMac, 0.025% MACs, 160, 160, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(320, 0.000% Params, 327.68 KMac, 0.000% MACs, 160, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 163.84 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(102.4 k, 0.034% Params, 104.86 MMac, 0.034% MACs, 160, 640, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "    )\n",
      "    (fusion_block): ConvBlock(\n",
      "      283.52 k, 0.094% Params, 291.31 MMac, 0.094% MACs, \n",
      "      (conv1): Conv1d(102.4 k, 0.034% Params, 104.86 MMac, 0.034% MACs, 640, 160, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(320, 0.000% Params, 327.68 KMac, 0.000% MACs, 160, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 163.84 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(76.8 k, 0.025% Params, 78.64 MMac, 0.025% MACs, 160, 160, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(320, 0.000% Params, 327.68 KMac, 0.000% MACs, 160, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 163.84 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(102.4 k, 0.034% Params, 104.86 MMac, 0.034% MACs, 160, 640, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "    )\n",
      "    (squeeze_block): FCUDown(\n",
      "      250.37 k, 0.083% Params, 257.95 MMac, 0.083% MACs, \n",
      "      (conv_project): Conv1d(247.3 k, 0.082% Params, 253.23 MMac, 0.082% MACs, 160, 1536, kernel_size=(1,), stride=(1,))\n",
      "      (sample_pooling): AvgPool1d(0, 0.000% Params, 1.57 MMac, 0.001% MACs, kernel_size=(1,), stride=(1,), padding=(0,))\n",
      "      (ln): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (act): GELU(0, 0.000% Params, 1.57 MMac, 0.001% MACs, approximate='none')\n",
      "    )\n",
      "    (expand_block): FCUUp(\n",
      "      246.24 k, 0.081% Params, 252.31 MMac, 0.081% MACs, \n",
      "      (conv_project): Conv1d(245.92 k, 0.081% Params, 251.82 MMac, 0.081% MACs, 1536, 160, kernel_size=(1,), stride=(1,))\n",
      "      (bn): BatchNorm1d(320, 0.000% Params, 327.68 KMac, 0.000% MACs, 160, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(0, 0.000% Params, 163.84 KMac, 0.000% MACs, )\n",
      "    )\n",
      "    (trans_block): Block(\n",
      "      18.89 M, 6.249% Params, 19.36 GMac, 6.246% MACs, \n",
      "      (norm1): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        9.44 M, 3.123% Params, 9.67 GMac, 3.122% MACs, \n",
      "        (qkv): Linear(7.08 M, 2.342% Params, 7.25 GMac, 2.341% MACs, in_features=1536, out_features=4608, bias=False)\n",
      "        (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "        (proj): Linear(2.36 M, 0.781% Params, 2.42 GMac, 0.781% MACs, in_features=1536, out_features=1536, bias=True)\n",
      "        (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "      (norm2): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        9.44 M, 3.124% Params, 9.68 GMac, 3.124% MACs, \n",
      "        (fc1): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.562% MACs, in_features=1536, out_features=3072, bias=True)\n",
      "        (act): GELU(0, 0.000% Params, 3.15 MMac, 0.001% MACs, approximate='none')\n",
      "        (fc2): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.561% MACs, in_features=3072, out_features=1536, bias=True)\n",
      "        (drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_trans_5): ConvTransBlock(\n",
      "    22.75 M, 7.528% Params, 23.32 GMac, 7.525% MACs, \n",
      "    (cnn_block): ConvBlock(\n",
      "      1.75 M, 0.578% Params, 1.79 GMac, 0.578% MACs, \n",
      "      (conv1): Conv1d(204.8 k, 0.068% Params, 209.72 MMac, 0.068% MACs, 640, 320, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(307.2 k, 0.102% Params, 314.57 MMac, 0.101% MACs, 320, 320, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(409.6 k, 0.136% Params, 419.43 MMac, 0.135% MACs, 320, 1280, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(2.56 k, 0.001% Params, 2.62 MMac, 0.001% MACs, 1280, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 1.31 MMac, 0.000% MACs, inplace=True)\n",
      "      (residual_conv): Conv1d(819.2 k, 0.271% Params, 838.86 MMac, 0.271% MACs, 640, 1280, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (residual_bn): BatchNorm1d(2.56 k, 0.001% Params, 2.62 MMac, 0.001% MACs, 1280, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (fusion_block): ConvBlock(\n",
      "      1.13 M, 0.374% Params, 1.16 GMac, 0.374% MACs, \n",
      "      (conv1): Conv1d(409.6 k, 0.136% Params, 419.43 MMac, 0.135% MACs, 1280, 320, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(307.2 k, 0.102% Params, 314.57 MMac, 0.101% MACs, 320, 320, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(409.6 k, 0.136% Params, 419.43 MMac, 0.135% MACs, 320, 1280, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(2.56 k, 0.001% Params, 2.62 MMac, 0.001% MACs, 1280, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 1.31 MMac, 0.000% MACs, inplace=True)\n",
      "    )\n",
      "    (squeeze_block): FCUDown(\n",
      "      496.13 k, 0.164% Params, 509.61 MMac, 0.164% MACs, \n",
      "      (conv_project): Conv1d(493.06 k, 0.163% Params, 504.89 MMac, 0.163% MACs, 320, 1536, kernel_size=(1,), stride=(1,))\n",
      "      (sample_pooling): AvgPool1d(0, 0.000% Params, 1.57 MMac, 0.001% MACs, kernel_size=(1,), stride=(1,), padding=(0,))\n",
      "      (ln): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (act): GELU(0, 0.000% Params, 1.57 MMac, 0.001% MACs, approximate='none')\n",
      "    )\n",
      "    (expand_block): FCUUp(\n",
      "      492.48 k, 0.163% Params, 504.63 MMac, 0.163% MACs, \n",
      "      (conv_project): Conv1d(491.84 k, 0.163% Params, 503.64 MMac, 0.163% MACs, 1536, 320, kernel_size=(1,), stride=(1,))\n",
      "      (bn): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, )\n",
      "    )\n",
      "    (trans_block): Block(\n",
      "      18.89 M, 6.249% Params, 19.36 GMac, 6.246% MACs, \n",
      "      (norm1): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        9.44 M, 3.123% Params, 9.67 GMac, 3.122% MACs, \n",
      "        (qkv): Linear(7.08 M, 2.342% Params, 7.25 GMac, 2.341% MACs, in_features=1536, out_features=4608, bias=False)\n",
      "        (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "        (proj): Linear(2.36 M, 0.781% Params, 2.42 GMac, 0.781% MACs, in_features=1536, out_features=1536, bias=True)\n",
      "        (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "      (norm2): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        9.44 M, 3.124% Params, 9.68 GMac, 3.124% MACs, \n",
      "        (fc1): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.562% MACs, in_features=1536, out_features=3072, bias=True)\n",
      "        (act): GELU(0, 0.000% Params, 3.15 MMac, 0.001% MACs, approximate='none')\n",
      "        (fc2): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.561% MACs, in_features=3072, out_features=1536, bias=True)\n",
      "        (drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_trans_6): ConvTransBlock(\n",
      "    22.14 M, 7.324% Params, 22.69 GMac, 7.322% MACs, \n",
      "    (cnn_block): ConvBlock(\n",
      "      1.13 M, 0.374% Params, 1.16 GMac, 0.374% MACs, \n",
      "      (conv1): Conv1d(409.6 k, 0.136% Params, 419.43 MMac, 0.135% MACs, 1280, 320, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(307.2 k, 0.102% Params, 314.57 MMac, 0.101% MACs, 320, 320, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(409.6 k, 0.136% Params, 419.43 MMac, 0.135% MACs, 320, 1280, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(2.56 k, 0.001% Params, 2.62 MMac, 0.001% MACs, 1280, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 1.31 MMac, 0.000% MACs, inplace=True)\n",
      "    )\n",
      "    (fusion_block): ConvBlock(\n",
      "      1.13 M, 0.374% Params, 1.16 GMac, 0.374% MACs, \n",
      "      (conv1): Conv1d(409.6 k, 0.136% Params, 419.43 MMac, 0.135% MACs, 1280, 320, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(307.2 k, 0.102% Params, 314.57 MMac, 0.101% MACs, 320, 320, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(409.6 k, 0.136% Params, 419.43 MMac, 0.135% MACs, 320, 1280, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(2.56 k, 0.001% Params, 2.62 MMac, 0.001% MACs, 1280, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 1.31 MMac, 0.000% MACs, inplace=True)\n",
      "    )\n",
      "    (squeeze_block): FCUDown(\n",
      "      496.13 k, 0.164% Params, 509.61 MMac, 0.164% MACs, \n",
      "      (conv_project): Conv1d(493.06 k, 0.163% Params, 504.89 MMac, 0.163% MACs, 320, 1536, kernel_size=(1,), stride=(1,))\n",
      "      (sample_pooling): AvgPool1d(0, 0.000% Params, 1.57 MMac, 0.001% MACs, kernel_size=(1,), stride=(1,), padding=(0,))\n",
      "      (ln): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (act): GELU(0, 0.000% Params, 1.57 MMac, 0.001% MACs, approximate='none')\n",
      "    )\n",
      "    (expand_block): FCUUp(\n",
      "      492.48 k, 0.163% Params, 504.63 MMac, 0.163% MACs, \n",
      "      (conv_project): Conv1d(491.84 k, 0.163% Params, 503.64 MMac, 0.163% MACs, 1536, 320, kernel_size=(1,), stride=(1,))\n",
      "      (bn): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, )\n",
      "    )\n",
      "    (trans_block): Block(\n",
      "      18.89 M, 6.249% Params, 19.36 GMac, 6.246% MACs, \n",
      "      (norm1): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        9.44 M, 3.123% Params, 9.67 GMac, 3.122% MACs, \n",
      "        (qkv): Linear(7.08 M, 2.342% Params, 7.25 GMac, 2.341% MACs, in_features=1536, out_features=4608, bias=False)\n",
      "        (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "        (proj): Linear(2.36 M, 0.781% Params, 2.42 GMac, 0.781% MACs, in_features=1536, out_features=1536, bias=True)\n",
      "        (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "      (norm2): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        9.44 M, 3.124% Params, 9.68 GMac, 3.124% MACs, \n",
      "        (fc1): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.562% MACs, in_features=1536, out_features=3072, bias=True)\n",
      "        (act): GELU(0, 0.000% Params, 3.15 MMac, 0.001% MACs, approximate='none')\n",
      "        (fc2): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.561% MACs, in_features=3072, out_features=1536, bias=True)\n",
      "        (drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_trans_7): ConvTransBlock(\n",
      "    22.14 M, 7.324% Params, 22.69 GMac, 7.322% MACs, \n",
      "    (cnn_block): ConvBlock(\n",
      "      1.13 M, 0.374% Params, 1.16 GMac, 0.374% MACs, \n",
      "      (conv1): Conv1d(409.6 k, 0.136% Params, 419.43 MMac, 0.135% MACs, 1280, 320, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(307.2 k, 0.102% Params, 314.57 MMac, 0.101% MACs, 320, 320, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(409.6 k, 0.136% Params, 419.43 MMac, 0.135% MACs, 320, 1280, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(2.56 k, 0.001% Params, 2.62 MMac, 0.001% MACs, 1280, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 1.31 MMac, 0.000% MACs, inplace=True)\n",
      "    )\n",
      "    (fusion_block): ConvBlock(\n",
      "      1.13 M, 0.374% Params, 1.16 GMac, 0.374% MACs, \n",
      "      (conv1): Conv1d(409.6 k, 0.136% Params, 419.43 MMac, 0.135% MACs, 1280, 320, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(307.2 k, 0.102% Params, 314.57 MMac, 0.101% MACs, 320, 320, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(409.6 k, 0.136% Params, 419.43 MMac, 0.135% MACs, 320, 1280, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(2.56 k, 0.001% Params, 2.62 MMac, 0.001% MACs, 1280, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 1.31 MMac, 0.000% MACs, inplace=True)\n",
      "    )\n",
      "    (squeeze_block): FCUDown(\n",
      "      496.13 k, 0.164% Params, 509.61 MMac, 0.164% MACs, \n",
      "      (conv_project): Conv1d(493.06 k, 0.163% Params, 504.89 MMac, 0.163% MACs, 320, 1536, kernel_size=(1,), stride=(1,))\n",
      "      (sample_pooling): AvgPool1d(0, 0.000% Params, 1.57 MMac, 0.001% MACs, kernel_size=(1,), stride=(1,), padding=(0,))\n",
      "      (ln): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (act): GELU(0, 0.000% Params, 1.57 MMac, 0.001% MACs, approximate='none')\n",
      "    )\n",
      "    (expand_block): FCUUp(\n",
      "      492.48 k, 0.163% Params, 504.63 MMac, 0.163% MACs, \n",
      "      (conv_project): Conv1d(491.84 k, 0.163% Params, 503.64 MMac, 0.163% MACs, 1536, 320, kernel_size=(1,), stride=(1,))\n",
      "      (bn): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, )\n",
      "    )\n",
      "    (trans_block): Block(\n",
      "      18.89 M, 6.249% Params, 19.36 GMac, 6.246% MACs, \n",
      "      (norm1): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        9.44 M, 3.123% Params, 9.67 GMac, 3.122% MACs, \n",
      "        (qkv): Linear(7.08 M, 2.342% Params, 7.25 GMac, 2.341% MACs, in_features=1536, out_features=4608, bias=False)\n",
      "        (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "        (proj): Linear(2.36 M, 0.781% Params, 2.42 GMac, 0.781% MACs, in_features=1536, out_features=1536, bias=True)\n",
      "        (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "      (norm2): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        9.44 M, 3.124% Params, 9.68 GMac, 3.124% MACs, \n",
      "        (fc1): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.562% MACs, in_features=1536, out_features=3072, bias=True)\n",
      "        (act): GELU(0, 0.000% Params, 3.15 MMac, 0.001% MACs, approximate='none')\n",
      "        (fc2): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.561% MACs, in_features=3072, out_features=1536, bias=True)\n",
      "        (drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_trans_8): ConvTransBlock(\n",
      "    22.14 M, 7.324% Params, 22.69 GMac, 7.322% MACs, \n",
      "    (cnn_block): ConvBlock(\n",
      "      1.13 M, 0.374% Params, 1.16 GMac, 0.374% MACs, \n",
      "      (conv1): Conv1d(409.6 k, 0.136% Params, 419.43 MMac, 0.135% MACs, 1280, 320, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(307.2 k, 0.102% Params, 314.57 MMac, 0.101% MACs, 320, 320, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(409.6 k, 0.136% Params, 419.43 MMac, 0.135% MACs, 320, 1280, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(2.56 k, 0.001% Params, 2.62 MMac, 0.001% MACs, 1280, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 1.31 MMac, 0.000% MACs, inplace=True)\n",
      "    )\n",
      "    (fusion_block): ConvBlock(\n",
      "      1.13 M, 0.374% Params, 1.16 GMac, 0.374% MACs, \n",
      "      (conv1): Conv1d(409.6 k, 0.136% Params, 419.43 MMac, 0.135% MACs, 1280, 320, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(307.2 k, 0.102% Params, 314.57 MMac, 0.101% MACs, 320, 320, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(409.6 k, 0.136% Params, 419.43 MMac, 0.135% MACs, 320, 1280, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(2.56 k, 0.001% Params, 2.62 MMac, 0.001% MACs, 1280, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 1.31 MMac, 0.000% MACs, inplace=True)\n",
      "    )\n",
      "    (squeeze_block): FCUDown(\n",
      "      496.13 k, 0.164% Params, 509.61 MMac, 0.164% MACs, \n",
      "      (conv_project): Conv1d(493.06 k, 0.163% Params, 504.89 MMac, 0.163% MACs, 320, 1536, kernel_size=(1,), stride=(1,))\n",
      "      (sample_pooling): AvgPool1d(0, 0.000% Params, 1.57 MMac, 0.001% MACs, kernel_size=(1,), stride=(1,), padding=(0,))\n",
      "      (ln): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (act): GELU(0, 0.000% Params, 1.57 MMac, 0.001% MACs, approximate='none')\n",
      "    )\n",
      "    (expand_block): FCUUp(\n",
      "      492.48 k, 0.163% Params, 504.63 MMac, 0.163% MACs, \n",
      "      (conv_project): Conv1d(491.84 k, 0.163% Params, 503.64 MMac, 0.163% MACs, 1536, 320, kernel_size=(1,), stride=(1,))\n",
      "      (bn): BatchNorm1d(640, 0.000% Params, 655.36 KMac, 0.000% MACs, 320, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(0, 0.000% Params, 327.68 KMac, 0.000% MACs, )\n",
      "    )\n",
      "    (trans_block): Block(\n",
      "      18.89 M, 6.249% Params, 19.36 GMac, 6.246% MACs, \n",
      "      (norm1): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        9.44 M, 3.123% Params, 9.67 GMac, 3.122% MACs, \n",
      "        (qkv): Linear(7.08 M, 2.342% Params, 7.25 GMac, 2.341% MACs, in_features=1536, out_features=4608, bias=False)\n",
      "        (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "        (proj): Linear(2.36 M, 0.781% Params, 2.42 GMac, 0.781% MACs, in_features=1536, out_features=1536, bias=True)\n",
      "        (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "      (norm2): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        9.44 M, 3.124% Params, 9.68 GMac, 3.124% MACs, \n",
      "        (fc1): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.562% MACs, in_features=1536, out_features=3072, bias=True)\n",
      "        (act): GELU(0, 0.000% Params, 3.15 MMac, 0.001% MACs, approximate='none')\n",
      "        (fc2): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.561% MACs, in_features=3072, out_features=1536, bias=True)\n",
      "        (drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_trans_9): ConvTransBlock(\n",
      "    32.35 M, 10.703% Params, 33.15 GMac, 10.697% MACs, \n",
      "    (cnn_block): ConvBlock(\n",
      "      6.98 M, 2.308% Params, 7.15 GMac, 2.306% MACs, \n",
      "      (conv1): Conv1d(819.2 k, 0.271% Params, 838.86 MMac, 0.271% MACs, 1280, 640, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(1.23 M, 0.407% Params, 1.26 GMac, 0.406% MACs, 640, 640, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(1.64 M, 0.542% Params, 1.68 GMac, 0.541% MACs, 640, 2560, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(5.12 k, 0.002% Params, 5.24 MMac, 0.002% MACs, 2560, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 2.62 MMac, 0.001% MACs, inplace=True)\n",
      "      (residual_conv): Conv1d(3.28 M, 1.084% Params, 3.36 GMac, 1.083% MACs, 1280, 2560, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (residual_bn): BatchNorm1d(5.12 k, 0.002% Params, 5.24 MMac, 0.002% MACs, 2560, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (fusion_block): ConvBlock(\n",
      "      4.51 M, 1.493% Params, 4.63 GMac, 1.492% MACs, \n",
      "      (conv1): Conv1d(1.64 M, 0.542% Params, 1.68 GMac, 0.541% MACs, 2560, 640, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(1.23 M, 0.407% Params, 1.26 GMac, 0.406% MACs, 640, 640, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(1.64 M, 0.542% Params, 1.68 GMac, 0.541% MACs, 640, 2560, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(5.12 k, 0.002% Params, 5.24 MMac, 0.002% MACs, 2560, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 2.62 MMac, 0.001% MACs, inplace=True)\n",
      "    )\n",
      "    (squeeze_block): FCUDown(\n",
      "      987.65 k, 0.327% Params, 1.01 GMac, 0.327% MACs, \n",
      "      (conv_project): Conv1d(984.58 k, 0.326% Params, 1.01 GMac, 0.325% MACs, 640, 1536, kernel_size=(1,), stride=(1,))\n",
      "      (sample_pooling): AvgPool1d(0, 0.000% Params, 1.57 MMac, 0.001% MACs, kernel_size=(1,), stride=(1,), padding=(0,))\n",
      "      (ln): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (act): GELU(0, 0.000% Params, 1.57 MMac, 0.001% MACs, approximate='none')\n",
      "    )\n",
      "    (expand_block): FCUUp(\n",
      "      984.96 k, 0.326% Params, 1.01 GMac, 0.326% MACs, \n",
      "      (conv_project): Conv1d(983.68 k, 0.325% Params, 1.01 GMac, 0.325% MACs, 1536, 640, kernel_size=(1,), stride=(1,))\n",
      "      (bn): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, )\n",
      "    )\n",
      "    (trans_block): Block(\n",
      "      18.89 M, 6.249% Params, 19.36 GMac, 6.246% MACs, \n",
      "      (norm1): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        9.44 M, 3.123% Params, 9.67 GMac, 3.122% MACs, \n",
      "        (qkv): Linear(7.08 M, 2.342% Params, 7.25 GMac, 2.341% MACs, in_features=1536, out_features=4608, bias=False)\n",
      "        (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "        (proj): Linear(2.36 M, 0.781% Params, 2.42 GMac, 0.781% MACs, in_features=1536, out_features=1536, bias=True)\n",
      "        (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "      (norm2): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        9.44 M, 3.124% Params, 9.68 GMac, 3.124% MACs, \n",
      "        (fc1): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.562% MACs, in_features=1536, out_features=3072, bias=True)\n",
      "        (act): GELU(0, 0.000% Params, 3.15 MMac, 0.001% MACs, approximate='none')\n",
      "        (fc2): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.561% MACs, in_features=3072, out_features=1536, bias=True)\n",
      "        (drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_trans_10): ConvTransBlock(\n",
      "    29.89 M, 9.888% Params, 30.63 GMac, 9.884% MACs, \n",
      "    (cnn_block): ConvBlock(\n",
      "      4.51 M, 1.493% Params, 4.63 GMac, 1.492% MACs, \n",
      "      (conv1): Conv1d(1.64 M, 0.542% Params, 1.68 GMac, 0.541% MACs, 2560, 640, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(1.23 M, 0.407% Params, 1.26 GMac, 0.406% MACs, 640, 640, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(1.64 M, 0.542% Params, 1.68 GMac, 0.541% MACs, 640, 2560, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(5.12 k, 0.002% Params, 5.24 MMac, 0.002% MACs, 2560, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 2.62 MMac, 0.001% MACs, inplace=True)\n",
      "    )\n",
      "    (fusion_block): ConvBlock(\n",
      "      4.51 M, 1.493% Params, 4.63 GMac, 1.492% MACs, \n",
      "      (conv1): Conv1d(1.64 M, 0.542% Params, 1.68 GMac, 0.541% MACs, 2560, 640, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(1.23 M, 0.407% Params, 1.26 GMac, 0.406% MACs, 640, 640, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(1.64 M, 0.542% Params, 1.68 GMac, 0.541% MACs, 640, 2560, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(5.12 k, 0.002% Params, 5.24 MMac, 0.002% MACs, 2560, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 2.62 MMac, 0.001% MACs, inplace=True)\n",
      "    )\n",
      "    (squeeze_block): FCUDown(\n",
      "      987.65 k, 0.327% Params, 1.01 GMac, 0.327% MACs, \n",
      "      (conv_project): Conv1d(984.58 k, 0.326% Params, 1.01 GMac, 0.325% MACs, 640, 1536, kernel_size=(1,), stride=(1,))\n",
      "      (sample_pooling): AvgPool1d(0, 0.000% Params, 1.57 MMac, 0.001% MACs, kernel_size=(1,), stride=(1,), padding=(0,))\n",
      "      (ln): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (act): GELU(0, 0.000% Params, 1.57 MMac, 0.001% MACs, approximate='none')\n",
      "    )\n",
      "    (expand_block): FCUUp(\n",
      "      984.96 k, 0.326% Params, 1.01 GMac, 0.326% MACs, \n",
      "      (conv_project): Conv1d(983.68 k, 0.325% Params, 1.01 GMac, 0.325% MACs, 1536, 640, kernel_size=(1,), stride=(1,))\n",
      "      (bn): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, )\n",
      "    )\n",
      "    (trans_block): Block(\n",
      "      18.89 M, 6.249% Params, 19.36 GMac, 6.246% MACs, \n",
      "      (norm1): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        9.44 M, 3.123% Params, 9.67 GMac, 3.122% MACs, \n",
      "        (qkv): Linear(7.08 M, 2.342% Params, 7.25 GMac, 2.341% MACs, in_features=1536, out_features=4608, bias=False)\n",
      "        (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "        (proj): Linear(2.36 M, 0.781% Params, 2.42 GMac, 0.781% MACs, in_features=1536, out_features=1536, bias=True)\n",
      "        (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "      (norm2): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        9.44 M, 3.124% Params, 9.68 GMac, 3.124% MACs, \n",
      "        (fc1): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.562% MACs, in_features=1536, out_features=3072, bias=True)\n",
      "        (act): GELU(0, 0.000% Params, 3.15 MMac, 0.001% MACs, approximate='none')\n",
      "        (fc2): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.561% MACs, in_features=3072, out_features=1536, bias=True)\n",
      "        (drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_trans_11): ConvTransBlock(\n",
      "    29.89 M, 9.888% Params, 30.63 GMac, 9.884% MACs, \n",
      "    (cnn_block): ConvBlock(\n",
      "      4.51 M, 1.493% Params, 4.63 GMac, 1.492% MACs, \n",
      "      (conv1): Conv1d(1.64 M, 0.542% Params, 1.68 GMac, 0.541% MACs, 2560, 640, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(1.23 M, 0.407% Params, 1.26 GMac, 0.406% MACs, 640, 640, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(1.64 M, 0.542% Params, 1.68 GMac, 0.541% MACs, 640, 2560, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(5.12 k, 0.002% Params, 5.24 MMac, 0.002% MACs, 2560, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 2.62 MMac, 0.001% MACs, inplace=True)\n",
      "    )\n",
      "    (fusion_block): ConvBlock(\n",
      "      4.51 M, 1.493% Params, 4.63 GMac, 1.492% MACs, \n",
      "      (conv1): Conv1d(1.64 M, 0.542% Params, 1.68 GMac, 0.541% MACs, 2560, 640, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(1.23 M, 0.407% Params, 1.26 GMac, 0.406% MACs, 640, 640, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(1.64 M, 0.542% Params, 1.68 GMac, 0.541% MACs, 640, 2560, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(5.12 k, 0.002% Params, 5.24 MMac, 0.002% MACs, 2560, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 2.62 MMac, 0.001% MACs, inplace=True)\n",
      "    )\n",
      "    (squeeze_block): FCUDown(\n",
      "      987.65 k, 0.327% Params, 1.01 GMac, 0.327% MACs, \n",
      "      (conv_project): Conv1d(984.58 k, 0.326% Params, 1.01 GMac, 0.325% MACs, 640, 1536, kernel_size=(1,), stride=(1,))\n",
      "      (sample_pooling): AvgPool1d(0, 0.000% Params, 1.57 MMac, 0.001% MACs, kernel_size=(1,), stride=(1,), padding=(0,))\n",
      "      (ln): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (act): GELU(0, 0.000% Params, 1.57 MMac, 0.001% MACs, approximate='none')\n",
      "    )\n",
      "    (expand_block): FCUUp(\n",
      "      984.96 k, 0.326% Params, 1.01 GMac, 0.326% MACs, \n",
      "      (conv_project): Conv1d(983.68 k, 0.325% Params, 1.01 GMac, 0.325% MACs, 1536, 640, kernel_size=(1,), stride=(1,))\n",
      "      (bn): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, )\n",
      "    )\n",
      "    (trans_block): Block(\n",
      "      18.89 M, 6.249% Params, 19.36 GMac, 6.246% MACs, \n",
      "      (norm1): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        9.44 M, 3.123% Params, 9.67 GMac, 3.122% MACs, \n",
      "        (qkv): Linear(7.08 M, 2.342% Params, 7.25 GMac, 2.341% MACs, in_features=1536, out_features=4608, bias=False)\n",
      "        (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "        (proj): Linear(2.36 M, 0.781% Params, 2.42 GMac, 0.781% MACs, in_features=1536, out_features=1536, bias=True)\n",
      "        (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "      (norm2): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        9.44 M, 3.124% Params, 9.68 GMac, 3.124% MACs, \n",
      "        (fc1): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.562% MACs, in_features=1536, out_features=3072, bias=True)\n",
      "        (act): GELU(0, 0.000% Params, 3.15 MMac, 0.001% MACs, approximate='none')\n",
      "        (fc2): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.561% MACs, in_features=3072, out_features=1536, bias=True)\n",
      "        (drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_trans_12): ConvTransBlock(\n",
      "    36.44 M, 12.058% Params, 37.35 GMac, 12.051% MACs, \n",
      "    (cnn_block): ConvBlock(\n",
      "      4.51 M, 1.493% Params, 4.63 GMac, 1.492% MACs, \n",
      "      (conv1): Conv1d(1.64 M, 0.542% Params, 1.68 GMac, 0.541% MACs, 2560, 640, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(1.23 M, 0.407% Params, 1.26 GMac, 0.406% MACs, 640, 640, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(1.64 M, 0.542% Params, 1.68 GMac, 0.541% MACs, 640, 2560, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(5.12 k, 0.002% Params, 5.24 MMac, 0.002% MACs, 2560, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 2.62 MMac, 0.001% MACs, inplace=True)\n",
      "    )\n",
      "    (fusion_block): ConvBlock(\n",
      "      11.07 M, 3.663% Params, 11.34 GMac, 3.659% MACs, \n",
      "      (conv1): Conv1d(1.64 M, 0.542% Params, 1.68 GMac, 0.541% MACs, 2560, 640, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv2): Conv1d(1.23 M, 0.407% Params, 1.26 GMac, 0.406% MACs, 640, 640, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (bn2): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, inplace=True)\n",
      "      (conv3): Conv1d(1.64 M, 0.542% Params, 1.68 GMac, 0.541% MACs, 640, 2560, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(5.12 k, 0.002% Params, 5.24 MMac, 0.002% MACs, 2560, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act3): ReLU(0, 0.000% Params, 2.62 MMac, 0.001% MACs, inplace=True)\n",
      "      (residual_conv): Conv1d(6.55 M, 2.168% Params, 6.71 GMac, 2.165% MACs, 2560, 2560, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (residual_bn): BatchNorm1d(5.12 k, 0.002% Params, 5.24 MMac, 0.002% MACs, 2560, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (squeeze_block): FCUDown(\n",
      "      987.65 k, 0.327% Params, 1.01 GMac, 0.327% MACs, \n",
      "      (conv_project): Conv1d(984.58 k, 0.326% Params, 1.01 GMac, 0.325% MACs, 640, 1536, kernel_size=(1,), stride=(1,))\n",
      "      (sample_pooling): AvgPool1d(0, 0.000% Params, 1.57 MMac, 0.001% MACs, kernel_size=(1,), stride=(1,), padding=(0,))\n",
      "      (ln): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (act): GELU(0, 0.000% Params, 1.57 MMac, 0.001% MACs, approximate='none')\n",
      "    )\n",
      "    (expand_block): FCUUp(\n",
      "      984.96 k, 0.326% Params, 1.01 GMac, 0.326% MACs, \n",
      "      (conv_project): Conv1d(983.68 k, 0.325% Params, 1.01 GMac, 0.325% MACs, 1536, 640, kernel_size=(1,), stride=(1,))\n",
      "      (bn): BatchNorm1d(1.28 k, 0.000% Params, 1.31 MMac, 0.000% MACs, 640, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU(0, 0.000% Params, 655.36 KMac, 0.000% MACs, )\n",
      "    )\n",
      "    (trans_block): Block(\n",
      "      18.89 M, 6.249% Params, 19.36 GMac, 6.246% MACs, \n",
      "      (norm1): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        9.44 M, 3.123% Params, 9.67 GMac, 3.122% MACs, \n",
      "        (qkv): Linear(7.08 M, 2.342% Params, 7.25 GMac, 2.341% MACs, in_features=1536, out_features=4608, bias=False)\n",
      "        (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "        (proj): Linear(2.36 M, 0.781% Params, 2.42 GMac, 0.781% MACs, in_features=1536, out_features=1536, bias=True)\n",
      "        (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "      (norm2): LayerNorm(3.07 k, 0.001% Params, 1.57 MMac, 0.001% MACs, (1536,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        9.44 M, 3.124% Params, 9.68 GMac, 3.124% MACs, \n",
      "        (fc1): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.562% MACs, in_features=1536, out_features=3072, bias=True)\n",
      "        (act): GELU(0, 0.000% Params, 3.15 MMac, 0.001% MACs, approximate='none')\n",
      "        (fc2): Linear(4.72 M, 1.562% Params, 4.84 GMac, 1.561% MACs, in_features=3072, out_features=1536, bias=True)\n",
      "        (drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Params: 302.24 M, MACs: 309.93 GMac\n"
     ]
    }
   ],
   "source": [
    "# de novo training, model params and MACs\n",
    "# BConformer, Conformer-12\n",
    "model = Conformer(in_chans=1280, num_classes=2, depth=12)\n",
    "# Conformer-9\n",
    "# model = Conformer(in_chans=1280, num_classes=2, depth=9)\n",
    "# Conformer-6\n",
    "# model = Conformer(in_chans=1280, num_classes=2, depth=6)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "macs, params = get_model_complexity_info(model, (1280, 1024), as_strings=True, verbose=False)\n",
    "print(f\"Params: {params}, MACs: {macs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c58910-370f-40d7-9601-58dfdfbfae13",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# training on pretrained models\n",
    "# model_name = \"...\" # model name, e.g. model_epoch104_AgIoU0.6534.pth\n",
    "# model_path = os.path.join(\"...\", model_name) # directory + model name\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = Conformer(in_chans=1280, num_classes=2)\n",
    "# state = torch.load(model_path, map_location=device, weights_only=False)\n",
    "# model.load_state_dict(state['model_state_dict'])\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee0feb-3920-4ab2-a112-f00b0e99c3ae",
   "metadata": {},
   "source": [
    "#### Training with checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e13ac-7aa7-4051-a20f-217a8fc146f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "threshold = 0.3\n",
    "epochs = 150\n",
    "all_metrics = []\n",
    "\n",
    "save_dir = \"...\" # directory saving models\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train_stats = train_one_epoch(model, criterion, dataloader, optimizer, device, epoch, scaler)\n",
    "    val_stats, _ = evaluate(dataloader, model, device, threshold)\n",
    "    \n",
    "    all_metrics.append(val_stats)\n",
    "    \n",
    "    print(f\"Train loss: {train_stats['loss']:.4f}\\n\")\n",
    "    \n",
    "    if 50 <= epoch + 1 <= 150:\n",
    "        save_path = os.path.join(save_dir, f\"model_epoch{epoch+1}_AgIoU{val_stats['AgIoU']:.4f}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'all_metrics': all_metrics,\n",
    "        }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4160aaaf-1b5a-4151-a298-5d45289f10f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# save training process (metrics at each epoch) to a csv.\n",
    "df = pd.DataFrame(all_metrics)\n",
    "df.insert(0, \"Epoch\", range(1, len(df) + 1))\n",
    "\n",
    "df.to_csv(\"....csv\", index=False)\n",
    "print(\"Successfully saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eecaa3-561e-44ec-86ec-7c22b8d34062",
   "metadata": {},
   "source": [
    "### 3.2 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68605936-5f03-4d6e-aa91-963c81427691",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# model_name = \"...\" # model name, e.g. model_epoch96_AgIoU0.6326.pth\n",
    "# model_path = os.path.join(\"...\", model_name) # checkpoints directory + model name\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = Conformer(in_chans=1280, num_classes=2)\n",
    "# state = torch.load(model_path, map_location=device, weights_only=False)\n",
    "# model.load_state_dict(state['model_state_dict'])\n",
    "# model.to(device)\n",
    "# thresholds = np.linspace(0.28, 0.32, 40)\n",
    "# collected_metrics = {}\n",
    "\n",
    "# for threshold in thresholds:\n",
    "#     with torch.no_grad():\n",
    "#         metrics, _ = evaluate_get_sample_iou(dataloader, model, device, threshold)\n",
    "#         for k, v in metrics.items():\n",
    "#             collected_metrics.setdefault(k, []).append(v)\n",
    "\n",
    "# # metrics (mean  std)\n",
    "# results_summary = {}\n",
    "# for k, v_list in collected_metrics.items():\n",
    "#     v_array = np.array(v_list)\n",
    "#     mean = np.mean(v_array)\n",
    "#     std = np.std(v_array)\n",
    "#     results_summary[k] = f\"{mean:.3f}  {std:.3f}\"\n",
    "\n",
    "# for k, v in results_summary.items():\n",
    "#     print(f\"{k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
