{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Vision Transformer\n",
    "##### This notebook contains all modules for training and evaluating the ViT model."
   ],
   "id": "1f6936d9da0794fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import torch\n",
    "from bconformer import embed, utils\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import gc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB import PDBParser, is_aa, Polypeptide\n",
    "from Bio.PDB.NeighborSearch import NeighborSearch\n",
    "from Bio.PDB.Selection import unfold_entities \n",
    "from bconformer.embed import Alphabet\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from torch.nn.init import trunc_normal_\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, matthews_corrcoef,\n",
    "    f1_score, precision_score, recall_score, accuracy_score,\n",
    "    brier_score_loss, log_loss\n",
    ")\n",
    "from typing import Iterable, Optional\n",
    "from timm.layers import DropPath\n",
    "from timm.data import Mixup\n",
    "from timm.utils import accuracy, ModelEma\n",
    "from ptflops import get_model_complexity_info"
   ],
   "id": "ef33718e3248db26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "three_to_one_dict = {\n",
    "    'ALA': 'A', 'CYS': 'C', 'ASP': 'D', 'GLU': 'E',\n",
    "    'PHE': 'F', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',\n",
    "    'LYS': 'K', 'LEU': 'L', 'MET': 'M', 'ASN': 'N',\n",
    "    'PRO': 'P', 'GLN': 'Q', 'ARG': 'R', 'SER': 'S',\n",
    "    'THR': 'T', 'VAL': 'V', 'TRP': 'W', 'TYR': 'Y',\n",
    "    'SEC': 'U', 'PYL': 'O', 'ASX': 'B', 'GLX': 'Z', 'UNK': 'X'\n",
    "}"
   ],
   "id": "2bdf81ab8f9921ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25e26b6-3ab1-4cbc-96bd-a7b7618ebe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_files = \"...\" # directory containing training (or evaluating) fastas\n",
    "pdb_files = \"...\" # directory containing training (or evaluating) pdbs\n",
    "\n",
    "num_fasta = len([f for f in os.listdir(fasta_files) if f.endswith('.fasta')])\n",
    "num_pdb = len([f for f in os.listdir(pdb_files) if f.endswith('.pdb')])\n",
    "num_fasta, num_pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b041c-4b31-4ee4-b44f-38b026b47666",
   "metadata": {},
   "source": [
    "### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89eeddd5-281c-42f7-b9d5-7bd38ec1da5a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_chains_from_fasta_name(fasta_name):\n",
    "    base = fasta_name.replace('.fasta', '')\n",
    "    parts = base.split('_')\n",
    "    ag_idx = parts.index('ag')\n",
    "    ab_idx = parts.index('ab')\n",
    "    antigen_chains = parts[ag_idx+1:ab_idx]\n",
    "    antibody_chains = parts[ab_idx+1:]\n",
    "    return antigen_chains, antibody_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adda3bae-5006-44d1-a1e8-cb50a8aa5e43",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_atoms(chains):\n",
    "    return [atom for chain in chains for atom in unfold_entities(chain, 'A') if atom.element != 'H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41c820da-0234-40e4-8ac7-f14868618616",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_epitope_labels(antigen_chain_objs, antibody_chain_objs):\n",
    "    antibody_atoms = get_atoms(antibody_chain_objs)\n",
    "    ns = NeighborSearch(antibody_atoms)\n",
    "    epitope_residues = set()\n",
    "\n",
    "    for chain in antigen_chain_objs:\n",
    "        for res in chain.get_residues():\n",
    "            if not is_aa(res):\n",
    "                continue\n",
    "            for atom in res:\n",
    "                if ns.search(atom.coord, 4):\n",
    "                    epitope_residues.add((chain.id, res.id))\n",
    "                    break\n",
    "\n",
    "    labels = []\n",
    "    for chain in antigen_chain_objs:\n",
    "        for res in chain.get_residues():\n",
    "            if not is_aa(res):\n",
    "                continue\n",
    "            label_val = 1 if (chain.id, res.id) in epitope_residues else 0\n",
    "            labels.append(label_val)\n",
    "    return torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c59629a-100e-4a3e-86ac-d29c33aafa38",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def esm_embed_sequences(sequences, model, alphabet, device):\n",
    "    embeddings = []\n",
    "    for seq in sequences:\n",
    "        batch = alphabet.get_batch_converter()([(\"protein\", seq)])\n",
    "        batch_labels, batch_strs, batch_tokens = batch\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        with torch.no_grad():\n",
    "            results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "        token_embeddings = results[\"representations\"][33]\n",
    "        # Remove BOS and EOS tokens\n",
    "        seq_embedding = token_embeddings[0, 1:-1].cpu()\n",
    "        embeddings.append(seq_embedding)\n",
    "    return torch.cat(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f3d590-c7c0-4e97-adf7-2f08d8ba6113",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class EpitopeDataset(Dataset):\n",
    "    def __init__(self, fasta_dir, pdb_dir, esm_model, esm_alphabet, device):\n",
    "        TOTAL_ANTIGEN_CHAINS = 0\n",
    "        TOTAL_ANTIBODY_CHAINS = 0\n",
    "        \n",
    "        self.fasta_dir = fasta_dir\n",
    "        self.pdb_dir = pdb_dir\n",
    "        self.esm_model = esm_model\n",
    "        self.esm_alphabet = esm_alphabet\n",
    "        self.device = device\n",
    "\n",
    "        self.fasta_files = sorted([f for f in os.listdir(fasta_dir) if f.endswith('.fasta')])\n",
    "        self.pdb_files = sorted([f for f in os.listdir(pdb_dir) if f.endswith('.pdb')])\n",
    "\n",
    "        self.antigen_len_cache = {}\n",
    "\n",
    "        total_ag = 0\n",
    "        total_ab = 0\n",
    "        for fasta_file in self.fasta_files:\n",
    "            ag_chains, ab_chains = parse_chains_from_fasta_name(fasta_file)\n",
    "            total_ag += len(ag_chains)\n",
    "            total_ab += len(ab_chains)\n",
    "\n",
    "        EpitopeDataset.TOTAL_ANTIGEN_CHAINS = total_ag\n",
    "        EpitopeDataset.TOTAL_ANTIBODY_CHAINS = total_ab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fasta_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.antigen_len_cache:\n",
    "            antigen_length = self.antigen_len_cache[idx]\n",
    "        else:\n",
    "            antigen_length = None\n",
    "\n",
    "        fasta_name = self.fasta_files[idx]\n",
    "        fasta_id = os.path.splitext(fasta_name)[0]\n",
    "\n",
    "        matched_pdb_file = None\n",
    "        for f in self.pdb_files:\n",
    "            if fasta_id in f:\n",
    "                matched_pdb_file = os.path.join(self.pdb_dir, f)\n",
    "                break\n",
    "\n",
    "        if matched_pdb_file is None:\n",
    "            raise ValueError(f\"No matching pdb file found for {fasta_name}\")\n",
    "\n",
    "        antigen_chains, antibody_chains = parse_chains_from_fasta_name(fasta_name)\n",
    "\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        structure = parser.get_structure(\"protein\", matched_pdb_file)\n",
    "        model = structure[0]\n",
    "\n",
    "        sorted_chain_ids = sorted([chain.id for chain in model])\n",
    "        assert len(sorted_chain_ids) == len(antigen_chains) + len(antibody_chains)\n",
    "\n",
    "        antigen_chain_ids = sorted_chain_ids[:len(antigen_chains)]\n",
    "        antibody_chain_ids = sorted_chain_ids[len(antigen_chains):]\n",
    "\n",
    "        antigen_chains_objs = [model[c] for c in antigen_chain_ids]\n",
    "        antibody_chains_objs = [model[c] for c in antibody_chain_ids]\n",
    "\n",
    "        # Antigen length\n",
    "        if antigen_length is None:\n",
    "            length = 0\n",
    "            for chain in antigen_chains_objs:\n",
    "                for residue in chain.get_residues():\n",
    "                    if is_aa(residue):\n",
    "                        length += 1\n",
    "            self.antigen_len_cache[idx] = length\n",
    "            antigen_length = length\n",
    "\n",
    "        # Antigen sequence\n",
    "        antigen_sequences = []\n",
    "        for chain in antigen_chains_objs:\n",
    "            seq = \"\"\n",
    "            for residue in chain.get_residues():\n",
    "                if is_aa(residue):\n",
    "                    try:\n",
    "                        resname = residue.get_resname()\n",
    "                        aa = three_to_one_dict.get(resname, 'X')\n",
    "                        seq += aa\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "            antigen_sequences.append(seq)\n",
    "\n",
    "        embedding = esm_embed_sequences(\n",
    "            antigen_sequences, self.esm_model, self.esm_alphabet, self.device\n",
    "        )\n",
    "\n",
    "        labels = get_epitope_labels(antigen_chains_objs, antibody_chains_objs)\n",
    "        mask = torch.ones(labels.shape[0], dtype=torch.bool)\n",
    "\n",
    "        return {\n",
    "            'embedding': embedding,\n",
    "            'labels': labels,\n",
    "            'mask': mask,\n",
    "            'antigen_length': antigen_length\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c4bde-5344-40bc-aeeb-bdab3df9e0b2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "esm_model, esm_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "esm_model = esm_model.to(device)\n",
    "esm_model.eval()\n",
    "\n",
    "dataset = EpitopeDataset(fasta_files, pdb_files, esm_model, esm_alphabet, device)\n",
    "\n",
    "print(\"Number of antigen chains:\", EpitopeDataset.TOTAL_ANTIGEN_CHAINS)\n",
    "print(\"Number of antibody chains:\", EpitopeDataset.TOTAL_ANTIBODY_CHAINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8b736ba-5090-44dd-850c-58f2837d701f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "max_seq_len = 1024\n",
    "\n",
    "def collate_fn_padding(batch):\n",
    "    batch_embeddings = []\n",
    "    batch_labels = []\n",
    "    batch_masks = []\n",
    "    attn_masks = []\n",
    "\n",
    "    for item in batch:\n",
    "        L = item['embedding'].shape[0]\n",
    "        pad_len = max_seq_len - L\n",
    "        if pad_len < 0:\n",
    "            continue\n",
    "\n",
    "        embedding = F.pad(item['embedding'], (0, 0, 0, pad_len), value=0)\n",
    "        labels = F.pad(item['labels'], (0, pad_len), value=-100)\n",
    "        mask = F.pad(item['mask'], (0, pad_len), value=0)\n",
    "        attn_mask = torch.cat([torch.ones(L), torch.zeros(pad_len)])\n",
    "\n",
    "        batch_embeddings.append(embedding)\n",
    "        batch_labels.append(labels)\n",
    "        batch_masks.append(mask)\n",
    "        attn_masks.append(attn_mask)\n",
    "\n",
    "    batch_embeddings = torch.stack(batch_embeddings)\n",
    "    batch_labels = torch.stack(batch_labels)\n",
    "    batch_masks = torch.stack(batch_masks)\n",
    "    attn_masks = torch.stack(attn_masks)\n",
    "\n",
    "    return {\n",
    "        \"embedding\": batch_embeddings,\n",
    "        \"labels\": batch_labels,\n",
    "        \"mask\": batch_masks,\n",
    "        \"attention_mask\": attn_masks\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1866aa22-3016-43c7-bcd5-67ac91fb92de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn_padding)\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    embedding = batch[\"embedding\"]        # shape: [B, max_len, 1280]\n",
    "    labels = batch[\"labels\"]              # shape: [B, max_len]\n",
    "    mask = batch[\"mask\"]                  # shape: [B, max_len]\n",
    "    attention_mask = batch[\"attention_mask\"]  # shape: [B, max_len]\n",
    "\n",
    "    print(f\"Embedding shape: {embedding.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Mask shape: {mask.shape}\")\n",
    "    print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "\n",
    "\n",
    "    print(\"\\n=== A sequence sample ===\")\n",
    "    print(f\"Embedding shape: {embedding[0].shape}\")  # [max_len, 1280]\n",
    "    print(f\"Embedding:\\n{embedding[0]}\")\n",
    "    print(f\"Labels:\\n{labels[0]}\")\n",
    "    print(f\"Mask:\\n{mask[0]}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd62caac-48e7-45cd-8f94-4fc56a6dc9b1",
   "metadata": {},
   "source": [
    "### 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38b16041-bd16-4b80-86d0-a8c110a6a7b2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding1D(nn.Module):\n",
    "    def __init__(self, in_dim=1280, patch_size=1, embed_dim=1536, seq_len=1024):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = seq_len // patch_size\n",
    "        self.proj = nn.Linear(in_dim * patch_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, C = x.shape  # [B, 1024, 1280]\n",
    "        # 切patch, reshape成 [B, num_patches, patch_size * C]\n",
    "        x = x.unfold(dimension=1, size=self.patch_size, step=self.patch_size)  # [B, num_patches, patch_size, C]\n",
    "        x = x.contiguous().view(B, self.num_patches, -1)  # flatten patch: [B, num_patches, patch_size * C]\n",
    "        x = self.proj(x)  # [B, num_patches, embed_dim]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "418f36c7-4d3c-4577-b6e2-e060c457a495",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "562f4bb3-7a0a-4f63-9c4f-0030c8c727e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=6, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)  # each [B, heads, N, head_dim]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # [B, heads, N, N]\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)  # [B, N, C]\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45ad9dfc-c16a-478d-a27b-f3ae9a318468",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, dim, num_heads=6, mlp_ratio=2., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, num_heads, qkv_bias, attn_drop, drop)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(dim, mlp_hidden_dim, drop=drop)\n",
    "        self.drop_path = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66046655-4967-448d-b846-63c9d5755164",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ViT_1D(nn.Module):\n",
    "    def __init__(self, in_dim=1280, seq_len=1024, patch_size=1, embed_dim=1536,\n",
    "                 depth=9, num_heads=12, num_classes=2, mlp_ratio=2.):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding1D(in_dim, patch_size, embed_dim, seq_len)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, seq_len // patch_size + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=0.)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim, num_heads, mlp_ratio)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=.02)\n",
    "        nn.init.trunc_normal_(self.head.weight, std=.02)\n",
    "        if self.head.bias is not None:\n",
    "            nn.init.zeros_(self.head.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)  # [B, num_patches, embed_dim]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, embed_dim]\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # [B, 1 + num_patches, embed_dim]\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)  # [B, 1 + num_patches, embed_dim]\n",
    "\n",
    "        token_features = x[:, 1:]  # Remove cls token: [B, num_patches, embed_dim]\n",
    "        out = self.head(token_features)  # [B, num_patches, num_classes]\n",
    "        out = out.permute(0, 2, 1)       # [B, num_classes, num_patches]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a37da-b1a5-4ff4-beed-e9d1d698306f",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "808987e7-981a-479c-a746-425b3448dd1b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n",
    "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
    "                    device: torch.device, epoch: int, loss_scaler=None, max_norm: float = 0,\n",
    "                    model_ema: Optional[object] = None, mixup_fn=None,\n",
    "                    set_training_mode=True):\n",
    "    model.train(set_training_mode)\n",
    "    if hasattr(criterion, 'train'):\n",
    "        criterion.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "    print_freq = 10\n",
    "\n",
    "    for batch in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        samples = batch['embedding'].to(device)\n",
    "        targets = batch['labels'].to(device)\n",
    "        mask = batch['mask'].to(device).bool()\n",
    "        # samples = samples.transpose(1, 2)\n",
    "        \n",
    "        with torch.cuda.amp.autocast(dtype=torch.float32):\n",
    "            output = model(samples)  # [B, num_classes, L]\n",
    "            loss = sequence_loss(output, targets, mask)\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        if not math.isfinite(loss_value):\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "\n",
    "        metric_logger.update(loss=loss_value)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8a80b46-045b-4cd7-8576-3267cd0dc8eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(data_loader, model, device, threshold=0.3):\n",
    "    model.eval()\n",
    "    true_positives = 0\n",
    "    union_positives = 0\n",
    "\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    sample_ious = []\n",
    "\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = \"Eval:\"\n",
    "    print_freq = 10\n",
    "    metric_logger.add_meter(\"mean_iou\", utils.SmoothedValue(window_size=1, fmt='{value:.4f}'))\n",
    "\n",
    "    for batch in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        samples = batch['embedding'].to(device)\n",
    "        targets = batch['labels'].to(device)\n",
    "        mask = batch['mask'].to(device).bool()\n",
    "        samples = samples.transpose(1, 2)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(samples)\n",
    "            probs = torch.softmax(output, dim=1)[:, 1, :]\n",
    "            preds = (probs > threshold).long()\n",
    "            preds = preds.masked_fill(~mask, 0)\n",
    "\n",
    "            for i in range(samples.shape[0]):\n",
    "                pred_i = preds[i]\n",
    "                target_i = targets[i]\n",
    "                mask_i = mask[i]\n",
    "\n",
    "                tp_i = ((pred_i == 1) & (target_i == 1) & mask_i).sum().item()\n",
    "                union_i = (((pred_i == 1) | (target_i == 1)) & mask_i).sum().item()\n",
    "                iou_i = tp_i / union_i if union_i > 0 else 0.0\n",
    "                sample_ious.append(iou_i)\n",
    "\n",
    "            tp = ((preds == 1) & (targets == 1) & mask).sum().item()\n",
    "            union = (((preds == 1) | (targets == 1)) & mask).sum().item()\n",
    "            true_positives += tp\n",
    "            union_positives += union\n",
    "\n",
    "            all_probs.append(probs[mask].cpu())\n",
    "            all_preds.append(preds[mask].cpu())\n",
    "            all_targets.append(targets[mask].cpu())\n",
    "\n",
    "            mean_iou_so_far = sum(sample_ious) / len(sample_ious)\n",
    "            metric_logger.update(mean_iou=mean_iou_so_far)\n",
    "\n",
    "    metric_logger.synchronize_between_processes()\n",
    "\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "    agiou = true_positives / union_positives if union_positives > 0 else 0.0\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(all_targets, all_probs)\n",
    "    except:\n",
    "        auc = float('nan')\n",
    "\n",
    "    try:\n",
    "        pr_auc = average_precision_score(all_targets, all_probs)\n",
    "    except:\n",
    "        pr_auc = float('nan')\n",
    "\n",
    "    try:\n",
    "        pcc = np.corrcoef(all_probs, all_targets)[0, 1]\n",
    "    except:\n",
    "        pcc = float('nan')\n",
    "\n",
    "    try:\n",
    "        brier = brier_score_loss(all_targets, all_probs)\n",
    "    except:\n",
    "        brier = float('nan')\n",
    "\n",
    "    try:\n",
    "        bce = log_loss(all_targets, all_probs, labels=[0, 1])\n",
    "    except:\n",
    "        bce = float('nan')\n",
    "\n",
    "    results = {\n",
    "        \"AgIoU\": round(agiou, 4),\n",
    "        \"Precision\": round(precision_score(all_targets, all_preds, zero_division=0), 4),\n",
    "        \"Recall\": round(recall_score(all_targets, all_preds, zero_division=0), 4),\n",
    "        \"F1\": round(f1_score(all_targets, all_preds, zero_division=0), 4),\n",
    "        \"MCC\": round(matthews_corrcoef(all_targets, all_preds), 4),\n",
    "        \"Accuracy\": round(accuracy_score(all_targets, all_preds), 4),\n",
    "        \"AUC\": round(auc, 4),\n",
    "        \"PR-AUC\": round(pr_auc, 4),\n",
    "        \"PCC\": round(pcc, 4),\n",
    "        \"Brier\": round(brier, 4),\n",
    "        \"BCE\": round(bce, 4)\n",
    "    }\n",
    "\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    for k, v in results.items():\n",
    "        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "    return results, sample_ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "946ebf00-0089-47c8-9c42-7d6f44c6ae87",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sequence_loss(pred, target, mask):\n",
    "    \"\"\"\n",
    "    pred: [B, C, L]\n",
    "    target: [B, L]\n",
    "    mask: [B, L] (bool)\n",
    "    \"\"\"\n",
    "    B, C, L = pred.shape\n",
    "    pred = pred.transpose(1, 2).reshape(-1, C)      # [B*L, C]\n",
    "    target = target.reshape(-1)                     # [B*L]\n",
    "    mask = mask.reshape(-1)                         # [B*L], bool\n",
    "\n",
    "    loss = F.cross_entropy(pred, target, reduction='none')  # [B*L]\n",
    "    loss = loss[mask].mean()  # only valid positions\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dac9df4-4067-4789-8e70-f34e8db66feb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def criterion(output, target, mask):\n",
    "    return sequence_loss(output, target, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a497559-11a6-432e-ae70-695f353c81fa",
   "metadata": {},
   "source": [
    "### 3.1 Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d8530a-8122-441e-b01f-fa2e2d7b44c1",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a11960e-1dc3-4069-8807-9329926d95c7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 173.57 M\n",
      "MACs: 176.32 GMac\n"
     ]
    }
   ],
   "source": [
    "# ViT-9\n",
    "model = ViT_1D(depth=9)\n",
    "# ViT-12\n",
    "# model = ViT_1D(depth=12)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "input_shape = (1024, 1280)\n",
    "with torch.cuda.device(0):\n",
    "    macs, params = get_model_complexity_info(model, input_shape, as_strings=True,\n",
    "                                             print_per_layer_stat=False, verbose=False)\n",
    "\n",
    "print(f\"Params: {params}\")\n",
    "print(f\"MACs: {macs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02c58910-370f-40d7-9601-58dfdfbfae13",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# training on pretrained models\n",
    "# model_name = \"...\" # model name\n",
    "# model_path = os.path.join(\"...\", model_name) # directory + model name\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = Conformer(in_chans=1280, num_classes=2)\n",
    "# state = torch.load(model_path, map_location=device, weights_only=False)\n",
    "# model.load_state_dict(state['model_state_dict'])\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee0feb-3920-4ab2-a112-f00b0e99c3ae",
   "metadata": {},
   "source": [
    "#### Training with checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e13ac-7aa7-4051-a20f-217a8fc146f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "threshold = 0.3\n",
    "epochs = 150\n",
    "all_metrics = []\n",
    "\n",
    "save_dir = \"...\" # directory saving models\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train_stats = train_one_epoch(model, criterion, dataloader, optimizer, device, epoch, scaler)\n",
    "    val_stats, _ = evaluate(dataloader, model, device, threshold)\n",
    "    \n",
    "    all_metrics.append(val_stats)\n",
    "    \n",
    "    print(f\"Train loss: {train_stats['loss']:.4f}\\n\")\n",
    "    \n",
    "    if 50 <= epoch + 1 <= 150:\n",
    "        save_path = os.path.join(save_dir, f\"model_epoch{epoch+1}_AgIoU{val_stats['AgIoU']:.4f}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'all_metrics': all_metrics,\n",
    "        }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4160aaaf-1b5a-4151-a298-5d45289f10f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# save training process (metrics at each epoch) to a csv.\n",
    "df = pd.DataFrame(all_metrics)\n",
    "df.insert(0, \"Epoch\", range(1, len(df) + 1))\n",
    "\n",
    "df.to_csv(\"....csv\", index=False)\n",
    "print(\"Successfully saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eecaa3-561e-44ec-86ec-7c22b8d34062",
   "metadata": {},
   "source": [
    "### 3.2 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68605936-5f03-4d6e-aa91-963c81427691",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# model_name = \"...\" # model name\n",
    "# model_path = os.path.join(\"...\", model_name) # checkpoints directory + model name\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = Conformer(in_chans=1280, num_classes=2)\n",
    "# state = torch.load(model_path, map_location=device, weights_only=False)\n",
    "# model.load_state_dict(state['model_state_dict'])\n",
    "# model.to(device)\n",
    "# thresholds = np.linspace(0.28, 0.32, 40)\n",
    "# collected_metrics = {}\n",
    "\n",
    "# for threshold in thresholds:\n",
    "#     with torch.no_grad():\n",
    "#         metrics, _ = evaluate_get_sample_iou(dataloader, model, device, threshold)\n",
    "#         for k, v in metrics.items():\n",
    "#             collected_metrics.setdefault(k, []).append(v)\n",
    "\n",
    "# # metrics (mean ± std)\n",
    "# results_summary = {}\n",
    "# for k, v_list in collected_metrics.items():\n",
    "#     v_array = np.array(v_list)\n",
    "#     mean = np.mean(v_array)\n",
    "#     std = np.std(v_array)\n",
    "#     results_summary[k] = f\"{mean:.3f} ± {std:.3f}\"\n",
    "\n",
    "# for k, v in results_summary.items():\n",
    "#     print(f\"{k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
