{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Resnet\n",
    "##### This notebook contains all modules for training and evaluating the Resnet model."
   ],
   "id": "dd95d00e10ed4563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import torch\n",
    "from bconformer import utils\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import gc\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB import PDBParser, is_aa\n",
    "from Bio.PDB.NeighborSearch import NeighborSearch\n",
    "from Bio.PDB.Selection import unfold_entities \n",
    "from bconformer.embed import Alphabet\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, matthews_corrcoef,\n",
    "    f1_score, precision_score, recall_score, accuracy_score,\n",
    "    brier_score_loss, log_loss\n",
    ")\n",
    "from typing import Iterable, Optional\n",
    "from timm.layers import DropPath\n",
    "from timm.data import Mixup\n",
    "from ptflops import get_model_complexity_info"
   ],
   "id": "d6e03db9fd8ddf0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "three_to_one_dict = {\n",
    "    'ALA': 'A', 'CYS': 'C', 'ASP': 'D', 'GLU': 'E',\n",
    "    'PHE': 'F', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',\n",
    "    'LYS': 'K', 'LEU': 'L', 'MET': 'M', 'ASN': 'N',\n",
    "    'PRO': 'P', 'GLN': 'Q', 'ARG': 'R', 'SER': 'S',\n",
    "    'THR': 'T', 'VAL': 'V', 'TRP': 'W', 'TYR': 'Y',\n",
    "    'SEC': 'U', 'PYL': 'O', 'ASX': 'B', 'GLX': 'Z', 'UNK': 'X'\n",
    "}"
   ],
   "id": "f2a5705fa884810e"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f25e26b6-3ab1-4cbc-96bd-a7b7618ebe4b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1080)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasta_files = \"...\" # directory containing training (or evaluating) fastas\n",
    "pdb_files = \"...\" # directory containing training (or evaluating) pdbs\n",
    "\n",
    "num_fasta = len([f for f in os.listdir(fasta_files) if f.endswith('.fasta')])\n",
    "num_pdb = len([f for f in os.listdir(pdb_files) if f.endswith('.pdb')])\n",
    "num_fasta, num_pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b041c-4b31-4ee4-b44f-38b026b47666",
   "metadata": {},
   "source": [
    "### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89eeddd5-281c-42f7-b9d5-7bd38ec1da5a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_chains_from_fasta_name(fasta_name):\n",
    "    base = fasta_name.replace('.fasta', '')\n",
    "    parts = base.split('_')\n",
    "    ag_idx = parts.index('ag')\n",
    "    ab_idx = parts.index('ab')\n",
    "    antigen_chains = parts[ag_idx+1:ab_idx]\n",
    "    antibody_chains = parts[ab_idx+1:]\n",
    "    return antigen_chains, antibody_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adda3bae-5006-44d1-a1e8-cb50a8aa5e43",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_atoms(chains):\n",
    "    return [atom for chain in chains for atom in unfold_entities(chain, 'A') if atom.element != 'H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41c820da-0234-40e4-8ac7-f14868618616",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_epitope_labels(antigen_chain_objs, antibody_chain_objs):\n",
    "    antibody_atoms = get_atoms(antibody_chain_objs)\n",
    "    ns = NeighborSearch(antibody_atoms)\n",
    "    epitope_residues = set()\n",
    "\n",
    "    for chain in antigen_chain_objs:\n",
    "        for res in chain.get_residues():\n",
    "            if not is_aa(res):\n",
    "                continue\n",
    "            for atom in res:\n",
    "                if ns.search(atom.coord, 4):\n",
    "                    epitope_residues.add((chain.id, res.id))\n",
    "                    break\n",
    "\n",
    "    labels = []\n",
    "    for chain in antigen_chain_objs:\n",
    "        for res in chain.get_residues():\n",
    "            if not is_aa(res):\n",
    "                continue\n",
    "            label_val = 1 if (chain.id, res.id) in epitope_residues else 0\n",
    "            labels.append(label_val)\n",
    "    return torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c59629a-100e-4a3e-86ac-d29c33aafa38",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def esm_embed_sequences(sequences, model, alphabet, device):\n",
    "    embeddings = []\n",
    "    for seq in sequences:\n",
    "        batch = alphabet.get_batch_converter()([(\"protein\", seq)])\n",
    "        batch_labels, batch_strs, batch_tokens = batch\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        with torch.no_grad():\n",
    "            results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "        token_embeddings = results[\"representations\"][33]\n",
    "        # Remove BOS and EOS tokens\n",
    "        seq_embedding = token_embeddings[0, 1:-1].cpu()\n",
    "        embeddings.append(seq_embedding)\n",
    "    return torch.cat(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0f3d590-c7c0-4e97-adf7-2f08d8ba6113",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class EpitopeDataset(Dataset):\n",
    "    def __init__(self, fasta_dir, pdb_dir, esm_model, esm_alphabet, device):\n",
    "        TOTAL_ANTIGEN_CHAINS = 0\n",
    "        TOTAL_ANTIBODY_CHAINS = 0\n",
    "        \n",
    "        self.fasta_dir = fasta_dir\n",
    "        self.pdb_dir = pdb_dir\n",
    "        self.esm_model = esm_model\n",
    "        self.esm_alphabet = esm_alphabet\n",
    "        self.device = device\n",
    "\n",
    "        self.fasta_files = sorted([f for f in os.listdir(fasta_dir) if f.endswith('.fasta')])\n",
    "        self.pdb_files = sorted([f for f in os.listdir(pdb_dir) if f.endswith('.pdb')])\n",
    "\n",
    "        self.antigen_len_cache = {}\n",
    "\n",
    "        total_ag = 0\n",
    "        total_ab = 0\n",
    "        for fasta_file in self.fasta_files:\n",
    "            ag_chains, ab_chains = parse_chains_from_fasta_name(fasta_file)\n",
    "            total_ag += len(ag_chains)\n",
    "            total_ab += len(ab_chains)\n",
    "\n",
    "        EpitopeDataset.TOTAL_ANTIGEN_CHAINS = total_ag\n",
    "        EpitopeDataset.TOTAL_ANTIBODY_CHAINS = total_ab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fasta_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.antigen_len_cache:\n",
    "            antigen_length = self.antigen_len_cache[idx]\n",
    "        else:\n",
    "            antigen_length = None\n",
    "\n",
    "        fasta_name = self.fasta_files[idx]\n",
    "        fasta_id = os.path.splitext(fasta_name)[0]\n",
    "\n",
    "        matched_pdb_file = None\n",
    "        for f in self.pdb_files:\n",
    "            if fasta_id in f:\n",
    "                matched_pdb_file = os.path.join(self.pdb_dir, f)\n",
    "                break\n",
    "\n",
    "        if matched_pdb_file is None:\n",
    "            raise ValueError(f\"No matching pdb file found for {fasta_name}\")\n",
    "\n",
    "        antigen_chains, antibody_chains = parse_chains_from_fasta_name(fasta_name)\n",
    "\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        structure = parser.get_structure(\"protein\", matched_pdb_file)\n",
    "        model = structure[0]\n",
    "\n",
    "        sorted_chain_ids = sorted([chain.id for chain in model])\n",
    "        assert len(sorted_chain_ids) == len(antigen_chains) + len(antibody_chains)\n",
    "\n",
    "        antigen_chain_ids = sorted_chain_ids[:len(antigen_chains)]\n",
    "        antibody_chain_ids = sorted_chain_ids[len(antigen_chains):]\n",
    "\n",
    "        antigen_chains_objs = [model[c] for c in antigen_chain_ids]\n",
    "        antibody_chains_objs = [model[c] for c in antibody_chain_ids]\n",
    "\n",
    "        # Antigen length\n",
    "        if antigen_length is None:\n",
    "            length = 0\n",
    "            for chain in antigen_chains_objs:\n",
    "                for residue in chain.get_residues():\n",
    "                    if is_aa(residue):\n",
    "                        length += 1\n",
    "            self.antigen_len_cache[idx] = length\n",
    "            antigen_length = length\n",
    "\n",
    "        # Antigen sequence\n",
    "        antigen_sequences = []\n",
    "        for chain in antigen_chains_objs:\n",
    "            seq = \"\"\n",
    "            for residue in chain.get_residues():\n",
    "                if is_aa(residue):\n",
    "                    try:\n",
    "                        resname = residue.get_resname()\n",
    "                        aa = three_to_one_dict.get(resname, 'X')\n",
    "                        seq += aa\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "            antigen_sequences.append(seq)\n",
    "\n",
    "        embedding = esm_embed_sequences(\n",
    "            antigen_sequences, self.esm_model, self.esm_alphabet, self.device\n",
    "        )\n",
    "\n",
    "        labels = get_epitope_labels(antigen_chains_objs, antibody_chains_objs)\n",
    "        mask = torch.ones(labels.shape[0], dtype=torch.bool)\n",
    "\n",
    "        return {\n",
    "            'embedding': embedding,\n",
    "            'labels': labels,\n",
    "            'mask': mask,\n",
    "            'antigen_length': antigen_length\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "797c4bde-5344-40bc-aeeb-bdab3df9e0b2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of antigen chains: 1338\n",
      "Number of antibody chains: 2160\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "esm_model, esm_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "esm_model = esm_model.to(device)\n",
    "esm_model.eval()\n",
    "\n",
    "dataset = EpitopeDataset(fasta_files, pdb_files, esm_model, esm_alphabet, device)\n",
    "\n",
    "print(\"Number of antigen chains:\", EpitopeDataset.TOTAL_ANTIGEN_CHAINS)\n",
    "print(\"Number of antibody chains:\", EpitopeDataset.TOTAL_ANTIBODY_CHAINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8b736ba-5090-44dd-850c-58f2837d701f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "max_seq_len = 1024\n",
    "\n",
    "def collate_fn_padding(batch):\n",
    "    batch_embeddings = []\n",
    "    batch_labels = []\n",
    "    batch_masks = []\n",
    "    attn_masks = []\n",
    "\n",
    "    for item in batch:\n",
    "        L = item['embedding'].shape[0]\n",
    "        pad_len = max_seq_len - L\n",
    "        if pad_len < 0:\n",
    "            continue\n",
    "\n",
    "        embedding = F.pad(item['embedding'], (0, 0, 0, pad_len), value=0)\n",
    "        labels = F.pad(item['labels'], (0, pad_len), value=-100)\n",
    "        mask = F.pad(item['mask'], (0, pad_len), value=0)\n",
    "        attn_mask = torch.cat([torch.ones(L), torch.zeros(pad_len)])\n",
    "\n",
    "        batch_embeddings.append(embedding)\n",
    "        batch_labels.append(labels)\n",
    "        batch_masks.append(mask)\n",
    "        attn_masks.append(attn_mask)\n",
    "\n",
    "    batch_embeddings = torch.stack(batch_embeddings)\n",
    "    batch_labels = torch.stack(batch_labels)\n",
    "    batch_masks = torch.stack(batch_masks)\n",
    "    attn_masks = torch.stack(attn_masks)\n",
    "\n",
    "    return {\n",
    "        \"embedding\": batch_embeddings,\n",
    "        \"labels\": batch_labels,\n",
    "        \"mask\": batch_masks,\n",
    "        \"attention_mask\": attn_masks\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1866aa22-3016-43c7-bcd5-67ac91fb92de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([8, 1024, 1280])\n",
      "Labels shape: torch.Size([8, 1024])\n",
      "Mask shape: torch.Size([8, 1024])\n",
      "Attention mask shape: torch.Size([8, 1024])\n",
      "\n",
      "=== A sequence sample ===\n",
      "Embedding shape: torch.Size([1024, 1280])\n",
      "Embedding:\n",
      "tensor([[ 0.0356,  0.1105, -0.0785,  ..., -0.0085, -0.0299, -0.0332],\n",
      "        [-0.0795,  0.0628,  0.0026,  ..., -0.0044, -0.0738,  0.0011],\n",
      "        [-0.0048,  0.0952, -0.0529,  ..., -0.0361,  0.0523, -0.1638],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "Labels:\n",
      "tensor([   0,    0,    0,  ..., -100, -100, -100])\n",
      "Mask:\n",
      "tensor([ True,  True,  True,  ..., False, False, False])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn_padding)\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    embedding = batch[\"embedding\"]        # shape: [B, max_len, 1280]\n",
    "    labels = batch[\"labels\"]              # shape: [B, max_len]\n",
    "    mask = batch[\"mask\"]                  # shape: [B, max_len]\n",
    "    attention_mask = batch[\"attention_mask\"]  # shape: [B, max_len]\n",
    "\n",
    "    print(f\"Embedding shape: {embedding.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Mask shape: {mask.shape}\")\n",
    "    print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "\n",
    "\n",
    "    print(\"\\n=== A sequence sample ===\")\n",
    "    print(f\"Embedding shape: {embedding[0].shape}\")  # [max_len, 1280]\n",
    "    print(f\"Embedding:\\n{embedding[0]}\")\n",
    "    print(f\"Labels:\\n{labels[0]}\")\n",
    "    print(f\"Mask:\\n{mask[0]}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd62caac-48e7-45cd-8f94-4fc56a6dc9b1",
   "metadata": {},
   "source": [
    "### 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38b16041-bd16-4b80-86d0-a8c110a6a7b2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Bottleneck1D(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "418f36c7-4d3c-4577-b6e2-e060c457a495",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, block, layers, in_channels=1280, num_classes=2):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        self.inplanes = 320\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels, 320, kernel_size=7, stride=1, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(320)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 160,  layers[0])\n",
    "        self.layer2 = self._make_layer(block, 320, layers[1])\n",
    "        self.layer3 = self._make_layer(block, 640, layers[2])\n",
    "        self.layer4 = self._make_layer(block, 1280, layers[3])\n",
    "\n",
    "        self.classifier = nn.Conv1d(1280 * block.expansion, num_classes, kernel_size=1)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        out_channels = planes * block.expansion\n",
    "\n",
    "        if self.inplanes != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x: [B, 1280, L]\n",
    "\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.classifier(x)      # [B, num_classes, L]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a37da-b1a5-4ff4-beed-e9d1d698306f",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "808987e7-981a-479c-a746-425b3448dd1b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n",
    "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
    "                    device: torch.device, epoch: int, loss_scaler=None, max_norm: float = 0,\n",
    "                    model_ema: Optional[object] = None, mixup_fn=None,\n",
    "                    set_training_mode=True):\n",
    "    model.train(set_training_mode)\n",
    "    if hasattr(criterion, 'train'):\n",
    "        criterion.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "    print_freq = 10\n",
    "\n",
    "    for batch in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        samples = batch['embedding'].to(device)\n",
    "        targets = batch['labels'].to(device)\n",
    "        mask = batch['mask'].to(device).bool()\n",
    "        samples = samples.transpose(1, 2)\n",
    "        \n",
    "\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float32):\n",
    "            output = model(samples)  # [B, num_classes, L]\n",
    "            loss = sequence_loss(output, targets, mask)\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        if not math.isfinite(loss_value):\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "\n",
    "        metric_logger.update(loss=loss_value)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8a80b46-045b-4cd7-8576-3267cd0dc8eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(data_loader, model, device, threshold=0.3):\n",
    "    model.eval()\n",
    "    true_positives = 0\n",
    "    union_positives = 0\n",
    "\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    sample_ious = []\n",
    "\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = \"Eval:\"\n",
    "    print_freq = 10\n",
    "    metric_logger.add_meter(\"mean_iou\", utils.SmoothedValue(window_size=1, fmt='{value:.4f}'))\n",
    "\n",
    "    for batch in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        samples = batch['embedding'].to(device)\n",
    "        targets = batch['labels'].to(device)\n",
    "        mask = batch['mask'].to(device).bool()\n",
    "        samples = samples.transpose(1, 2)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(samples)\n",
    "            probs = torch.softmax(output, dim=1)[:, 1, :]\n",
    "            preds = (probs > threshold).long()\n",
    "            preds = preds.masked_fill(~mask, 0)\n",
    "\n",
    "            for i in range(samples.shape[0]):\n",
    "                pred_i = preds[i]\n",
    "                target_i = targets[i]\n",
    "                mask_i = mask[i]\n",
    "\n",
    "                tp_i = ((pred_i == 1) & (target_i == 1) & mask_i).sum().item()\n",
    "                union_i = (((pred_i == 1) | (target_i == 1)) & mask_i).sum().item()\n",
    "                iou_i = tp_i / union_i if union_i > 0 else 0.0\n",
    "                sample_ious.append(iou_i)\n",
    "\n",
    "            tp = ((preds == 1) & (targets == 1) & mask).sum().item()\n",
    "            union = (((preds == 1) | (targets == 1)) & mask).sum().item()\n",
    "            true_positives += tp\n",
    "            union_positives += union\n",
    "\n",
    "            all_probs.append(probs[mask].cpu())\n",
    "            all_preds.append(preds[mask].cpu())\n",
    "            all_targets.append(targets[mask].cpu())\n",
    "\n",
    "            mean_iou_so_far = sum(sample_ious) / len(sample_ious)\n",
    "            metric_logger.update(mean_iou=mean_iou_so_far)\n",
    "\n",
    "    metric_logger.synchronize_between_processes()\n",
    "\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "    agiou = true_positives / union_positives if union_positives > 0 else 0.0\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(all_targets, all_probs)\n",
    "    except:\n",
    "        auc = float('nan')\n",
    "\n",
    "    try:\n",
    "        pr_auc = average_precision_score(all_targets, all_probs)\n",
    "    except:\n",
    "        pr_auc = float('nan')\n",
    "\n",
    "    try:\n",
    "        pcc = np.corrcoef(all_probs, all_targets)[0, 1]\n",
    "    except:\n",
    "        pcc = float('nan')\n",
    "\n",
    "    try:\n",
    "        brier = brier_score_loss(all_targets, all_probs)\n",
    "    except:\n",
    "        brier = float('nan')\n",
    "\n",
    "    try:\n",
    "        bce = log_loss(all_targets, all_probs, labels=[0, 1])\n",
    "    except:\n",
    "        bce = float('nan')\n",
    "\n",
    "    results = {\n",
    "        \"AgIoU\": round(agiou, 4),\n",
    "        \"Precision\": round(precision_score(all_targets, all_preds, zero_division=0), 4),\n",
    "        \"Recall\": round(recall_score(all_targets, all_preds, zero_division=0), 4),\n",
    "        \"F1\": round(f1_score(all_targets, all_preds, zero_division=0), 4),\n",
    "        \"MCC\": round(matthews_corrcoef(all_targets, all_preds), 4),\n",
    "        \"Accuracy\": round(accuracy_score(all_targets, all_preds), 4),\n",
    "        \"AUC\": round(auc, 4),\n",
    "        \"PR-AUC\": round(pr_auc, 4),\n",
    "        \"PCC\": round(pcc, 4),\n",
    "        \"Brier\": round(brier, 4),\n",
    "        \"BCE\": round(bce, 4)\n",
    "    }\n",
    "\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    for k, v in results.items():\n",
    "        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "    return results, sample_ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "946ebf00-0089-47c8-9c42-7d6f44c6ae87",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sequence_loss(pred, target, mask):\n",
    "    \"\"\"\n",
    "    pred: [B, C, L]\n",
    "    target: [B, L]\n",
    "    mask: [B, L] (bool)\n",
    "    \"\"\"\n",
    "    B, C, L = pred.shape\n",
    "    pred = pred.transpose(1, 2).reshape(-1, C)      # [B*L, C]\n",
    "    target = target.reshape(-1)                     # [B*L]\n",
    "    mask = mask.reshape(-1)                         # [B*L], bool\n",
    "\n",
    "    loss = F.cross_entropy(pred, target, reduction='none')  # [B*L]\n",
    "    loss = loss[mask].mean()  # only valid positions\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6dac9df4-4067-4789-8e70-f34e8db66feb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def criterion(output, target, mask):\n",
    "    return sequence_loss(output, target, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a497559-11a6-432e-ae70-695f353c81fa",
   "metadata": {},
   "source": [
    "### 3.1 Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d8530a-8122-441e-b01f-fa2e2d7b44c1",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a11960e-1dc3-4069-8807-9329926d95c7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 179.24 M\n",
      "MACs: 183.8 GMac\n"
     ]
    }
   ],
   "source": [
    "# Resnet-101\n",
    "model = ResNet1D(block=Bottleneck1D, layers=[3, 4, 23, 3], in_channels=1280, num_classes=2)\n",
    "# Resnet-152\n",
    "# model = ResNet1D(block=Bottleneck1D, layers=[3, 8, 36, 3], in_channels=1280, num_classes=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "input_shape = (1280, 1024)\n",
    "with torch.cuda.device(0):\n",
    "    macs, params = get_model_complexity_info(model, input_shape, as_strings=True,\n",
    "                                             print_per_layer_stat=False, verbose=False)\n",
    "\n",
    "print(f\"Params: {params}\")\n",
    "print(f\"MACs: {macs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c58910-370f-40d7-9601-58dfdfbfae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on pretrained models\n",
    "# model_name = \"...\" # model name\n",
    "# model_path = os.path.join(\"...\", model_name) # directory + model name\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = Conformer(in_chans=1280, num_classes=2)\n",
    "# state = torch.load(model_path, map_location=device, weights_only=False)\n",
    "# model.load_state_dict(state['model_state_dict'])\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee0feb-3920-4ab2-a112-f00b0e99c3ae",
   "metadata": {},
   "source": [
    "#### Training with checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e13ac-7aa7-4051-a20f-217a8fc146f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "threshold = 0.3\n",
    "epochs = 150\n",
    "all_metrics = []\n",
    "\n",
    "save_dir = \"...\" # directory saving models\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train_stats = train_one_epoch(model, criterion, dataloader, optimizer, device, epoch, scaler)\n",
    "    val_stats, _ = evaluate(dataloader, model, device, threshold)\n",
    "    \n",
    "    all_metrics.append(val_stats)\n",
    "    \n",
    "    print(f\"Train loss: {train_stats['loss']:.4f}\\n\")\n",
    "    \n",
    "    if 50 <= epoch + 1 <= 150:\n",
    "        save_path = os.path.join(save_dir, f\"model_epoch{epoch+1}_AgIoU{val_stats['AgIoU']:.4f}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'all_metrics': all_metrics,\n",
    "        }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4160aaaf-1b5a-4151-a298-5d45289f10f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# save training process (metrics at each epoch) to a csv.\n",
    "df = pd.DataFrame(all_metrics)\n",
    "df.insert(0, \"Epoch\", range(1, len(df) + 1))\n",
    "\n",
    "df.to_csv(\"....csv\", index=False)\n",
    "print(\"Successfully saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eecaa3-561e-44ec-86ec-7c22b8d34062",
   "metadata": {},
   "source": [
    "### 3.2 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68605936-5f03-4d6e-aa91-963c81427691",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# model_name = \"...\" # model name\n",
    "# model_path = os.path.join(\"...\", model_name) # checkpoints directory + model name\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = Conformer(in_chans=1280, num_classes=2)\n",
    "# state = torch.load(model_path, map_location=device, weights_only=False)\n",
    "# model.load_state_dict(state['model_state_dict'])\n",
    "# model.to(device)\n",
    "# thresholds = np.linspace(0.28, 0.32, 40)\n",
    "# collected_metrics = {}\n",
    "\n",
    "# for threshold in thresholds:\n",
    "#     with torch.no_grad():\n",
    "#         metrics, _ = evaluate_get_sample_iou(dataloader, model, device, threshold)\n",
    "#         for k, v in metrics.items():\n",
    "#             collected_metrics.setdefault(k, []).append(v)\n",
    "\n",
    "# # metrics (mean ± std)\n",
    "# results_summary = {}\n",
    "# for k, v_list in collected_metrics.items():\n",
    "#     v_array = np.array(v_list)\n",
    "#     mean = np.mean(v_array)\n",
    "#     std = np.std(v_array)\n",
    "#     results_summary[k] = f\"{mean:.3f} ± {std:.3f}\"\n",
    "\n",
    "# for k, v in results_summary.items():\n",
    "#     print(f\"{k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
